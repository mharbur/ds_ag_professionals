<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 13 Machine Learning | Data Science for Agricultural Professionals</title>
  <meta name="description" content="Practical statistics for those involved in agronomy and related agricultural sciences." />
  <meta name="generator" content="bookdown 0.24 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 13 Machine Learning | Data Science for Agricultural Professionals" />
  <meta property="og:type" content="book" />
  <meta property="og:url" content="https://mharbur.github.io/data-science-for-agricultural-professionals/" />
  
  <meta property="og:description" content="Practical statistics for those involved in agronomy and related agricultural sciences." />
  <meta name="github-repo" content="https://github.com/mharbur/ds_ag_professionals" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 13 Machine Learning | Data Science for Agricultural Professionals" />
  
  <meta name="twitter:description" content="Practical statistics for those involved in agronomy and related agricultural sciences." />
  

<meta name="author" content="Marin L. Harbur" />


<meta name="date" content="2022-08-19" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="spatial-statistics.html"/>
<link rel="next" href="putting-it-all-together.html"/>
<script src="libs/header-attrs-2.14/header-attrs.js"></script>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>
<script src="libs/kePrint-0.0.1/kePrint.js"></script>
<link href="libs/lightable-0.0.1/lightable.css" rel="stylesheet" />
<script src="libs/htmlwidgets-1.5.4/htmlwidgets.js"></script>
<script src="libs/plotly-binding-4.10.0/plotly.js"></script>
<script src="libs/typedarray-0.1/typedarray.min.js"></script>
<link href="libs/crosstalk-1.2.0/css/crosstalk.min.css" rel="stylesheet" />
<script src="libs/crosstalk-1.2.0/js/crosstalk.min.js"></script>
<link href="libs/plotly-htmlwidgets-css-2.5.1/plotly-htmlwidgets.css" rel="stylesheet" />
<script src="libs/plotly-main-2.5.1/plotly-latest.min.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Data Science for Agricultural Professionals</a></li>

<li class="divider"></li>
<li><a href="index.html#preface">Preface<span></span></a>
<ul>
<li><a href="index.html#welcome">Welcome<span></span></a></li>
<li><a href="index.html#r-language">R-language<span></span></a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="population-statistics.html"><a href="population-statistics.html"><i class="fa fa-check"></i><b>1</b> Population Statistics<span></span></a>
<ul>
<li class="chapter" data-level="1.1" data-path="population-statistics.html"><a href="population-statistics.html#populations"><i class="fa fa-check"></i><b>1.1</b> Populations<span></span></a></li>
<li class="chapter" data-level="1.2" data-path="population-statistics.html"><a href="population-statistics.html#case-study-yield-map"><i class="fa fa-check"></i><b>1.2</b> Case Study: Yield Map<span></span></a></li>
<li class="chapter" data-level="1.3" data-path="population-statistics.html"><a href="population-statistics.html#distributions"><i class="fa fa-check"></i><b>1.3</b> Distributions<span></span></a>
<ul>
<li class="chapter" data-level="1.3.1" data-path="population-statistics.html"><a href="population-statistics.html#histograms"><i class="fa fa-check"></i><b>1.3.1</b> Histograms<span></span></a></li>
<li class="chapter" data-level="1.3.2" data-path="population-statistics.html"><a href="population-statistics.html#percentiles"><i class="fa fa-check"></i><b>1.3.2</b> Percentiles<span></span></a></li>
<li class="chapter" data-level="1.3.3" data-path="population-statistics.html"><a href="population-statistics.html#normal-distribution-model"><i class="fa fa-check"></i><b>1.3.3</b> Normal Distribution Model<span></span></a></li>
<li class="chapter" data-level="1.3.4" data-path="population-statistics.html"><a href="population-statistics.html#measures-of-center"><i class="fa fa-check"></i><b>1.3.4</b> Measures of Center<span></span></a></li>
<li class="chapter" data-level="1.3.5" data-path="population-statistics.html"><a href="population-statistics.html#measures-of-dispersion"><i class="fa fa-check"></i><b>1.3.5</b> Measures of Dispersion<span></span></a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2" data-path="distributions-and-probability.html"><a href="distributions-and-probability.html"><i class="fa fa-check"></i><b>2</b> Distributions and Probability<span></span></a>
<ul>
<li class="chapter" data-level="2.1" data-path="distributions-and-probability.html"><a href="distributions-and-probability.html#case-study"><i class="fa fa-check"></i><b>2.1</b> Case Study<span></span></a></li>
<li class="chapter" data-level="2.2" data-path="distributions-and-probability.html"><a href="distributions-and-probability.html#the-normal-distribution-model"><i class="fa fa-check"></i><b>2.2</b> The Normal Distribution Model<span></span></a>
<ul>
<li class="chapter" data-level="2.2.1" data-path="distributions-and-probability.html"><a href="distributions-and-probability.html#the-bell-curve"><i class="fa fa-check"></i><b>2.2.1</b> The Bell Curve<span></span></a></li>
<li class="chapter" data-level="2.2.2" data-path="distributions-and-probability.html"><a href="distributions-and-probability.html#distribution-and-probability"><i class="fa fa-check"></i><b>2.2.2</b> Distribution and Probability<span></span></a></li>
<li class="chapter" data-level="2.2.3" data-path="distributions-and-probability.html"><a href="distributions-and-probability.html#probability-and-the-normal-distribution-curve"><i class="fa fa-check"></i><b>2.2.3</b> Probability and the Normal Distribution Curve<span></span></a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="distributions-and-probability.html"><a href="distributions-and-probability.html#the-z-distribution"><i class="fa fa-check"></i><b>2.3</b> The Z-Distribution<span></span></a>
<ul>
<li class="chapter" data-level="2.3.1" data-path="distributions-and-probability.html"><a href="distributions-and-probability.html#important-numbers-95-and-5"><i class="fa fa-check"></i><b>2.3.1</b> Important Numbers: 95% and 5%<span></span></a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="sample-statistics.html"><a href="sample-statistics.html"><i class="fa fa-check"></i><b>3</b> Sample Statistics<span></span></a>
<ul>
<li class="chapter" data-level="3.1" data-path="sample-statistics.html"><a href="sample-statistics.html#samples"><i class="fa fa-check"></i><b>3.1</b> Samples<span></span></a></li>
<li class="chapter" data-level="3.2" data-path="sample-statistics.html"><a href="sample-statistics.html#case-study-1"><i class="fa fa-check"></i><b>3.2</b> Case Study<span></span></a></li>
<li class="chapter" data-level="3.3" data-path="sample-statistics.html"><a href="sample-statistics.html#distribution-of-sample-means"><i class="fa fa-check"></i><b>3.3</b> Distribution of Sample Means<span></span></a></li>
<li class="chapter" data-level="3.4" data-path="sample-statistics.html"><a href="sample-statistics.html#central-limit-theorem"><i class="fa fa-check"></i><b>3.4</b> Central Limit Theorem<span></span></a></li>
<li class="chapter" data-level="3.5" data-path="sample-statistics.html"><a href="sample-statistics.html#standard-error"><i class="fa fa-check"></i><b>3.5</b> Standard Error<span></span></a></li>
<li class="chapter" data-level="3.6" data-path="sample-statistics.html"><a href="sample-statistics.html#degrees-of-freedom"><i class="fa fa-check"></i><b>3.6</b> Degrees of Freedom<span></span></a></li>
<li class="chapter" data-level="3.7" data-path="sample-statistics.html"><a href="sample-statistics.html#the-t-distribution"><i class="fa fa-check"></i><b>3.7</b> The t-Distribution<span></span></a></li>
<li class="chapter" data-level="3.8" data-path="sample-statistics.html"><a href="sample-statistics.html#confidence-interval"><i class="fa fa-check"></i><b>3.8</b> Confidence Interval<span></span></a></li>
<li class="chapter" data-level="3.9" data-path="sample-statistics.html"><a href="sample-statistics.html#confidence-interval-and-probability"><i class="fa fa-check"></i><b>3.9</b> Confidence Interval and Probability<span></span></a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="two-treatment-comparisons.html"><a href="two-treatment-comparisons.html"><i class="fa fa-check"></i><b>4</b> Two-Treatment Comparisons<span></span></a>
<ul>
<li class="chapter" data-level="4.1" data-path="two-treatment-comparisons.html"><a href="two-treatment-comparisons.html#side-by-side-trials"><i class="fa fa-check"></i><b>4.1</b> Side-by-Side Trials<span></span></a></li>
<li class="chapter" data-level="4.2" data-path="two-treatment-comparisons.html"><a href="two-treatment-comparisons.html#blocked-design"><i class="fa fa-check"></i><b>4.2</b> Blocked Design<span></span></a></li>
<li class="chapter" data-level="4.3" data-path="two-treatment-comparisons.html"><a href="two-treatment-comparisons.html#case-study-2"><i class="fa fa-check"></i><b>4.3</b> Case Study<span></span></a></li>
<li class="chapter" data-level="4.4" data-path="two-treatment-comparisons.html"><a href="two-treatment-comparisons.html#confidence-interval-1"><i class="fa fa-check"></i><b>4.4</b> Confidence Interval<span></span></a></li>
<li class="chapter" data-level="4.5" data-path="two-treatment-comparisons.html"><a href="two-treatment-comparisons.html#t-test"><i class="fa fa-check"></i><b>4.5</b> T-Test<span></span></a></li>
<li class="chapter" data-level="4.6" data-path="two-treatment-comparisons.html"><a href="two-treatment-comparisons.html#conclusion"><i class="fa fa-check"></i><b>4.6</b> Conclusion<span></span></a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="understanding-statistical-tests.html"><a href="understanding-statistical-tests.html"><i class="fa fa-check"></i><b>5</b> Understanding Statistical Tests<span></span></a>
<ul>
<li class="chapter" data-level="5.1" data-path="understanding-statistical-tests.html"><a href="understanding-statistical-tests.html#research-question"><i class="fa fa-check"></i><b>5.1</b> Research Question<span></span></a></li>
<li class="chapter" data-level="5.2" data-path="understanding-statistical-tests.html"><a href="understanding-statistical-tests.html#the-model"><i class="fa fa-check"></i><b>5.2</b> The Model<span></span></a>
<ul>
<li class="chapter" data-level="5.2.1" data-path="understanding-statistical-tests.html"><a href="understanding-statistical-tests.html#treatment-effect"><i class="fa fa-check"></i><b>5.2.1</b> Treatment Effect<span></span></a></li>
<li class="chapter" data-level="5.2.2" data-path="understanding-statistical-tests.html"><a href="understanding-statistical-tests.html#error-effect"><i class="fa fa-check"></i><b>5.2.2</b> Error Effect<span></span></a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="understanding-statistical-tests.html"><a href="understanding-statistical-tests.html#hypotheses"><i class="fa fa-check"></i><b>5.3</b> Hypotheses<span></span></a></li>
<li class="chapter" data-level="5.4" data-path="understanding-statistical-tests.html"><a href="understanding-statistical-tests.html#p-value"><i class="fa fa-check"></i><b>5.4</b> P-Value<span></span></a></li>
<li class="chapter" data-level="5.5" data-path="understanding-statistical-tests.html"><a href="understanding-statistical-tests.html#the-p-value-and-errors"><i class="fa fa-check"></i><b>5.5</b> The P-Value and Errors<span></span></a></li>
<li class="chapter" data-level="5.6" data-path="understanding-statistical-tests.html"><a href="understanding-statistical-tests.html#one-sided-vs-two-sided-hypotheses"><i class="fa fa-check"></i><b>5.6</b> One-Sided vs Two-Sided Hypotheses<span></span></a></li>
<li class="chapter" data-level="5.7" data-path="understanding-statistical-tests.html"><a href="understanding-statistical-tests.html#exercise-linear-additive-model"><i class="fa fa-check"></i><b>5.7</b> Exercise: Linear Additive Model<span></span></a>
<ul>
<li class="chapter" data-level="5.7.1" data-path="understanding-statistical-tests.html"><a href="understanding-statistical-tests.html#case-study-barley-effects"><i class="fa fa-check"></i><b>5.7.1</b> Case Study: Barley Effects<span></span></a></li>
<li class="chapter" data-level="5.7.2" data-path="understanding-statistical-tests.html"><a href="understanding-statistical-tests.html#plotting-the-effects"><i class="fa fa-check"></i><b>5.7.2</b> Plotting the Effects<span></span></a></li>
<li class="chapter" data-level="5.7.3" data-path="understanding-statistical-tests.html"><a href="understanding-statistical-tests.html#examining-individual-plots"><i class="fa fa-check"></i><b>5.7.3</b> Examining Individual Plots<span></span></a></li>
<li class="chapter" data-level="5.7.4" data-path="understanding-statistical-tests.html"><a href="understanding-statistical-tests.html#practice-groudnut"><i class="fa fa-check"></i><b>5.7.4</b> Practice: Groudnut<span></span></a></li>
<li class="chapter" data-level="5.7.5" data-path="understanding-statistical-tests.html"><a href="understanding-statistical-tests.html#plotting-the-effects-1"><i class="fa fa-check"></i><b>5.7.5</b> Plotting the Effects<span></span></a></li>
</ul></li>
<li class="chapter" data-level="5.8" data-path="understanding-statistical-tests.html"><a href="understanding-statistical-tests.html#exercise-one-sided-hypotheses"><i class="fa fa-check"></i><b>5.8</b> Exercise: One-Sided Hypotheses<span></span></a>
<ul>
<li class="chapter" data-level="5.8.1" data-path="understanding-statistical-tests.html"><a href="understanding-statistical-tests.html#case-study-groundnut"><i class="fa fa-check"></i><b>5.8.1</b> Case Study: Groundnut<span></span></a></li>
<li class="chapter" data-level="5.8.2" data-path="understanding-statistical-tests.html"><a href="understanding-statistical-tests.html#one-sided-t-test"><i class="fa fa-check"></i><b>5.8.2</b> One-Sided T-Test<span></span></a></li>
<li class="chapter" data-level="5.8.3" data-path="understanding-statistical-tests.html"><a href="understanding-statistical-tests.html#practice-barley"><i class="fa fa-check"></i><b>5.8.3</b> Practice: Barley<span></span></a></li>
<li class="chapter" data-level="5.8.4" data-path="understanding-statistical-tests.html"><a href="understanding-statistical-tests.html#practice-strawberry"><i class="fa fa-check"></i><b>5.8.4</b> Practice: Strawberry<span></span></a></li>
</ul></li>
<li class="chapter" data-level="5.9" data-path="understanding-statistical-tests.html"><a href="understanding-statistical-tests.html#exercise-type-i-and-type-ii-errors"><i class="fa fa-check"></i><b>5.9</b> Exercise: Type I and Type II Errors<span></span></a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="multiple-treatment-trials.html"><a href="multiple-treatment-trials.html"><i class="fa fa-check"></i><b>6</b> Multiple Treatment Trials<span></span></a>
<ul>
<li class="chapter" data-level="6.1" data-path="multiple-treatment-trials.html"><a href="multiple-treatment-trials.html#case-study-3"><i class="fa fa-check"></i><b>6.1</b> Case Study<span></span></a></li>
<li class="chapter" data-level="6.2" data-path="multiple-treatment-trials.html"><a href="multiple-treatment-trials.html#the-linear-additive-model"><i class="fa fa-check"></i><b>6.2</b> The Linear Additive Model<span></span></a></li>
<li class="chapter" data-level="6.3" data-path="multiple-treatment-trials.html"><a href="multiple-treatment-trials.html#analysis-of-variance"><i class="fa fa-check"></i><b>6.3</b> Analysis of Variance<span></span></a></li>
<li class="chapter" data-level="6.4" data-path="multiple-treatment-trials.html"><a href="multiple-treatment-trials.html#the-f-statistic"><i class="fa fa-check"></i><b>6.4</b> The F statistic<span></span></a></li>
<li class="chapter" data-level="6.5" data-path="multiple-treatment-trials.html"><a href="multiple-treatment-trials.html#the-anova-table"><i class="fa fa-check"></i><b>6.5</b> The ANOVA Table<span></span></a>
<ul>
<li class="chapter" data-level="6.5.1" data-path="multiple-treatment-trials.html"><a href="multiple-treatment-trials.html#source-of-variation"><i class="fa fa-check"></i><b>6.5.1</b> Source of Variation<span></span></a></li>
<li class="chapter" data-level="6.5.2" data-path="multiple-treatment-trials.html"><a href="multiple-treatment-trials.html#sum-of-squares-1"><i class="fa fa-check"></i><b>6.5.2</b> Sum of Squares<span></span></a></li>
<li class="chapter" data-level="6.5.3" data-path="multiple-treatment-trials.html"><a href="multiple-treatment-trials.html#degrees-of-freedom-1"><i class="fa fa-check"></i><b>6.5.3</b> Degrees of Freedom<span></span></a></li>
<li class="chapter" data-level="6.5.4" data-path="multiple-treatment-trials.html"><a href="multiple-treatment-trials.html#mean-square"><i class="fa fa-check"></i><b>6.5.4</b> Mean Square<span></span></a></li>
<li class="chapter" data-level="6.5.5" data-path="multiple-treatment-trials.html"><a href="multiple-treatment-trials.html#f-value"><i class="fa fa-check"></i><b>6.5.5</b> F-Value<span></span></a></li>
<li class="chapter" data-level="6.5.6" data-path="multiple-treatment-trials.html"><a href="multiple-treatment-trials.html#p-value-1"><i class="fa fa-check"></i><b>6.5.6</b> P-value<span></span></a></li>
</ul></li>
<li class="chapter" data-level="6.6" data-path="multiple-treatment-trials.html"><a href="multiple-treatment-trials.html#visualizing-how-the-anova-table-relates-to-variance"><i class="fa fa-check"></i><b>6.6</b> Visualizing How the Anova Table Relates to Variance<span></span></a>
<ul>
<li class="chapter" data-level="6.6.1" data-path="multiple-treatment-trials.html"><a href="multiple-treatment-trials.html#case-study-barley"><i class="fa fa-check"></i><b>6.6.1</b> Case Study: Barley<span></span></a></li>
<li class="chapter" data-level="6.6.2" data-path="multiple-treatment-trials.html"><a href="multiple-treatment-trials.html#anova"><i class="fa fa-check"></i><b>6.6.2</b> ANOVA<span></span></a></li>
<li class="chapter" data-level="6.6.3" data-path="multiple-treatment-trials.html"><a href="multiple-treatment-trials.html#calculating-the-coefficient-of-variation"><i class="fa fa-check"></i><b>6.6.3</b> Calculating the Coefficient of Variation<span></span></a></li>
<li class="chapter" data-level="6.6.4" data-path="multiple-treatment-trials.html"><a href="multiple-treatment-trials.html#practice-beet-data"><i class="fa fa-check"></i><b>6.6.4</b> Practice: Beet Data<span></span></a></li>
<li class="chapter" data-level="6.6.5" data-path="multiple-treatment-trials.html"><a href="multiple-treatment-trials.html#practice-potato-data"><i class="fa fa-check"></i><b>6.6.5</b> Practice: Potato Data<span></span></a></li>
</ul></li>
<li class="chapter" data-level="6.7" data-path="multiple-treatment-trials.html"><a href="multiple-treatment-trials.html#exercise-treatment-means"><i class="fa fa-check"></i><b>6.7</b> Exercise: “Treatment Means”<span></span></a>
<ul>
<li class="chapter" data-level="6.7.1" data-path="multiple-treatment-trials.html"><a href="multiple-treatment-trials.html#case-study-barley-1"><i class="fa fa-check"></i><b>6.7.1</b> Case Study: Barley<span></span></a></li>
<li class="chapter" data-level="6.7.2" data-path="multiple-treatment-trials.html"><a href="multiple-treatment-trials.html#calculating-treatment-means"><i class="fa fa-check"></i><b>6.7.2</b> Calculating Treatment Means<span></span></a></li>
<li class="chapter" data-level="6.7.3" data-path="multiple-treatment-trials.html"><a href="multiple-treatment-trials.html#plotting-the-means"><i class="fa fa-check"></i><b>6.7.3</b> Plotting the Means<span></span></a></li>
<li class="chapter" data-level="6.7.4" data-path="multiple-treatment-trials.html"><a href="multiple-treatment-trials.html#practice-beet-data-1"><i class="fa fa-check"></i><b>6.7.4</b> Practice: Beet Data<span></span></a></li>
<li class="chapter" data-level="6.7.5" data-path="multiple-treatment-trials.html"><a href="multiple-treatment-trials.html#practice-potato-data-1"><i class="fa fa-check"></i><b>6.7.5</b> Practice: Potato Data<span></span></a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="7" data-path="multiple-treatment-designs.html"><a href="multiple-treatment-designs.html"><i class="fa fa-check"></i><b>7</b> Multiple Treatment Designs<span></span></a>
<ul>
<li class="chapter" data-level="7.1" data-path="multiple-treatment-designs.html"><a href="multiple-treatment-designs.html#randomized-complete-block-design"><i class="fa fa-check"></i><b>7.1</b> Randomized Complete Block Design<span></span></a>
<ul>
<li class="chapter" data-level="7.1.1" data-path="multiple-treatment-designs.html"><a href="multiple-treatment-designs.html#case-study-randomized-complete-block-design"><i class="fa fa-check"></i><b>7.1.1</b> Case Study: Randomized Complete Block Design<span></span></a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="multiple-treatment-designs.html"><a href="multiple-treatment-designs.html#factorial-design"><i class="fa fa-check"></i><b>7.2</b> Factorial Design<span></span></a>
<ul>
<li class="chapter" data-level="7.2.1" data-path="multiple-treatment-designs.html"><a href="multiple-treatment-designs.html#case-study-1-1"><i class="fa fa-check"></i><b>7.2.1</b> Case Study 1<span></span></a></li>
<li class="chapter" data-level="7.2.2" data-path="multiple-treatment-designs.html"><a href="multiple-treatment-designs.html#case-study-2-1"><i class="fa fa-check"></i><b>7.2.2</b> Case Study 2<span></span></a></li>
<li class="chapter" data-level="7.2.3" data-path="multiple-treatment-designs.html"><a href="multiple-treatment-designs.html#discussing-interactions"><i class="fa fa-check"></i><b>7.2.3</b> Discussing Interactions<span></span></a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="multiple-treatment-designs.html"><a href="multiple-treatment-designs.html#split-plot-design"><i class="fa fa-check"></i><b>7.3</b> Split-Plot Design<span></span></a></li>
<li class="chapter" data-level="7.4" data-path="multiple-treatment-designs.html"><a href="multiple-treatment-designs.html#linear-additive-model-3"><i class="fa fa-check"></i><b>7.4</b> Linear Additive Model<span></span></a></li>
<li class="chapter" data-level="7.5" data-path="multiple-treatment-designs.html"><a href="multiple-treatment-designs.html#conclusion-1"><i class="fa fa-check"></i><b>7.5</b> Conclusion<span></span></a></li>
<li class="chapter" data-level="7.6" data-path="multiple-treatment-designs.html"><a href="multiple-treatment-designs.html#exercise-randomized-complete-block-design"><i class="fa fa-check"></i><b>7.6</b> Exercise: Randomized Complete Block Design<span></span></a>
<ul>
<li class="chapter" data-level="7.6.1" data-path="multiple-treatment-designs.html"><a href="multiple-treatment-designs.html#case-study-4"><i class="fa fa-check"></i><b>7.6.1</b> Case Study<span></span></a></li>
<li class="chapter" data-level="7.6.2" data-path="multiple-treatment-designs.html"><a href="multiple-treatment-designs.html#anova-1"><i class="fa fa-check"></i><b>7.6.2</b> ANOVA<span></span></a></li>
<li class="chapter" data-level="7.6.3" data-path="multiple-treatment-designs.html"><a href="multiple-treatment-designs.html#plotting-the-results"><i class="fa fa-check"></i><b>7.6.3</b> Plotting the Results<span></span></a></li>
<li class="chapter" data-level="7.6.4" data-path="multiple-treatment-designs.html"><a href="multiple-treatment-designs.html#practice"><i class="fa fa-check"></i><b>7.6.4</b> Practice<span></span></a></li>
</ul></li>
<li class="chapter" data-level="7.7" data-path="multiple-treatment-designs.html"><a href="multiple-treatment-designs.html#exercise-factorial-anova"><i class="fa fa-check"></i><b>7.7</b> Exercise: Factorial ANOVA<span></span></a>
<ul>
<li class="chapter" data-level="7.7.1" data-path="multiple-treatment-designs.html"><a href="multiple-treatment-designs.html#case-study-biochar"><i class="fa fa-check"></i><b>7.7.1</b> Case Study: Biochar<span></span></a></li>
<li class="chapter" data-level="7.7.2" data-path="multiple-treatment-designs.html"><a href="multiple-treatment-designs.html#anova-2"><i class="fa fa-check"></i><b>7.7.2</b> ANOVA<span></span></a></li>
<li class="chapter" data-level="7.7.3" data-path="multiple-treatment-designs.html"><a href="multiple-treatment-designs.html#interaction-plots"><i class="fa fa-check"></i><b>7.7.3</b> Interaction Plots<span></span></a></li>
<li class="chapter" data-level="7.7.4" data-path="multiple-treatment-designs.html"><a href="multiple-treatment-designs.html#testing-factors-individually"><i class="fa fa-check"></i><b>7.7.4</b> Testing Factors Individually<span></span></a></li>
<li class="chapter" data-level="7.7.5" data-path="multiple-treatment-designs.html"><a href="multiple-treatment-designs.html#bar-plots"><i class="fa fa-check"></i><b>7.7.5</b> Bar Plots<span></span></a></li>
<li class="chapter" data-level="7.7.6" data-path="multiple-treatment-designs.html"><a href="multiple-treatment-designs.html#practice-1"><i class="fa fa-check"></i><b>7.7.6</b> Practice<span></span></a></li>
</ul></li>
<li class="chapter" data-level="7.8" data-path="multiple-treatment-designs.html"><a href="multiple-treatment-designs.html#exercise-split-plot-design"><i class="fa fa-check"></i><b>7.8</b> Exercise: Split-Plot Design<span></span></a>
<ul>
<li class="chapter" data-level="7.8.1" data-path="multiple-treatment-designs.html"><a href="multiple-treatment-designs.html#case-study-corn-soybean-systems-trial"><i class="fa fa-check"></i><b>7.8.1</b> Case Study: Corn-Soybean Systems Trial<span></span></a></li>
<li class="chapter" data-level="7.8.2" data-path="multiple-treatment-designs.html"><a href="multiple-treatment-designs.html#anova-4"><i class="fa fa-check"></i><b>7.8.2</b> ANOVA<span></span></a></li>
<li class="chapter" data-level="7.8.3" data-path="multiple-treatment-designs.html"><a href="multiple-treatment-designs.html#interaction-plot"><i class="fa fa-check"></i><b>7.8.3</b> Interaction Plot<span></span></a></li>
<li class="chapter" data-level="7.8.4" data-path="multiple-treatment-designs.html"><a href="multiple-treatment-designs.html#bar-plot"><i class="fa fa-check"></i><b>7.8.4</b> Bar plot<span></span></a></li>
<li class="chapter" data-level="7.8.5" data-path="multiple-treatment-designs.html"><a href="multiple-treatment-designs.html#practice-2"><i class="fa fa-check"></i><b>7.8.5</b> Practice<span></span></a></li>
</ul></li>
<li class="chapter" data-level="7.9" data-path="multiple-treatment-designs.html"><a href="multiple-treatment-designs.html#exercise-experimental-design"><i class="fa fa-check"></i><b>7.9</b> Exercise: Experimental Design<span></span></a>
<ul>
<li class="chapter" data-level="7.9.1" data-path="multiple-treatment-designs.html"><a href="multiple-treatment-designs.html#completely-randomized-design"><i class="fa fa-check"></i><b>7.9.1</b> Completely Randomized Design<span></span></a></li>
<li class="chapter" data-level="7.9.2" data-path="multiple-treatment-designs.html"><a href="multiple-treatment-designs.html#randomized-complete-block-design-1"><i class="fa fa-check"></i><b>7.9.2</b> Randomized Complete Block Design<span></span></a></li>
<li class="chapter" data-level="7.9.3" data-path="multiple-treatment-designs.html"><a href="multiple-treatment-designs.html#factorial-design-1"><i class="fa fa-check"></i><b>7.9.3</b> Factorial Design<span></span></a></li>
<li class="chapter" data-level="7.9.4" data-path="multiple-treatment-designs.html"><a href="multiple-treatment-designs.html#split-plot-design-1"><i class="fa fa-check"></i><b>7.9.4</b> Split-Plot Design<span></span></a></li>
<li class="chapter" data-level="7.9.5" data-path="multiple-treatment-designs.html"><a href="multiple-treatment-designs.html#practice-3"><i class="fa fa-check"></i><b>7.9.5</b> Practice<span></span></a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="8" data-path="means-separation-and-data-presentation.html"><a href="means-separation-and-data-presentation.html"><i class="fa fa-check"></i><b>8</b> Means Separation and Data Presentation<span></span></a>
<ul>
<li class="chapter" data-level="8.1" data-path="means-separation-and-data-presentation.html"><a href="means-separation-and-data-presentation.html#case-study-5"><i class="fa fa-check"></i><b>8.1</b> Case Study<span></span></a></li>
<li class="chapter" data-level="8.2" data-path="means-separation-and-data-presentation.html"><a href="means-separation-and-data-presentation.html#least-significant-difference"><i class="fa fa-check"></i><b>8.2</b> Least Significant Difference<span></span></a></li>
<li class="chapter" data-level="8.3" data-path="means-separation-and-data-presentation.html"><a href="means-separation-and-data-presentation.html#lsd-output-in-r"><i class="fa fa-check"></i><b>8.3</b> LSD Output in R<span></span></a>
<ul>
<li class="chapter" data-level="8.3.1" data-path="means-separation-and-data-presentation.html"><a href="means-separation-and-data-presentation.html#statistics-table"><i class="fa fa-check"></i><b>8.3.1</b> Statistics Table<span></span></a></li>
<li class="chapter" data-level="8.3.2" data-path="means-separation-and-data-presentation.html"><a href="means-separation-and-data-presentation.html#means-table"><i class="fa fa-check"></i><b>8.3.2</b> Means Table<span></span></a></li>
<li class="chapter" data-level="8.3.3" data-path="means-separation-and-data-presentation.html"><a href="means-separation-and-data-presentation.html#groups-table"><i class="fa fa-check"></i><b>8.3.3</b> Groups Table<span></span></a></li>
</ul></li>
<li class="chapter" data-level="8.4" data-path="means-separation-and-data-presentation.html"><a href="means-separation-and-data-presentation.html#comparisonwise-versus-experimentwise-error"><i class="fa fa-check"></i><b>8.4</b> Comparisonwise versus Experimentwise Error<span></span></a></li>
<li class="chapter" data-level="8.5" data-path="means-separation-and-data-presentation.html"><a href="means-separation-and-data-presentation.html#tukeys-honest-significant-difference"><i class="fa fa-check"></i><b>8.5</b> Tukey’s Honest Significant Difference<span></span></a></li>
<li class="chapter" data-level="8.6" data-path="means-separation-and-data-presentation.html"><a href="means-separation-and-data-presentation.html#linear-contrast"><i class="fa fa-check"></i><b>8.6</b> Linear Contrast<span></span></a>
<ul>
<li class="chapter" data-level="8.6.1" data-path="means-separation-and-data-presentation.html"><a href="means-separation-and-data-presentation.html#coefficients"><i class="fa fa-check"></i><b>8.6.1</b> Coefficients<span></span></a></li>
<li class="chapter" data-level="8.6.2" data-path="means-separation-and-data-presentation.html"><a href="means-separation-and-data-presentation.html#linear-contrasts-with-r"><i class="fa fa-check"></i><b>8.6.2</b> Linear Contrasts with R<span></span></a></li>
</ul></li>
<li class="chapter" data-level="8.7" data-path="means-separation-and-data-presentation.html"><a href="means-separation-and-data-presentation.html#means-presentation"><i class="fa fa-check"></i><b>8.7</b> Means Presentation<span></span></a>
<ul>
<li class="chapter" data-level="8.7.1" data-path="means-separation-and-data-presentation.html"><a href="means-separation-and-data-presentation.html#means-tables"><i class="fa fa-check"></i><b>8.7.1</b> Means Tables<span></span></a></li>
<li class="chapter" data-level="8.7.2" data-path="means-separation-and-data-presentation.html"><a href="means-separation-and-data-presentation.html#plotting-means"><i class="fa fa-check"></i><b>8.7.2</b> Plotting Means<span></span></a></li>
</ul></li>
<li class="chapter" data-level="8.8" data-path="means-separation-and-data-presentation.html"><a href="means-separation-and-data-presentation.html#exercise-lsd-and-tukeys-hsd"><i class="fa fa-check"></i><b>8.8</b> Exercise: LSD and Tukey’s HSD<span></span></a>
<ul>
<li class="chapter" data-level="8.8.1" data-path="means-separation-and-data-presentation.html"><a href="means-separation-and-data-presentation.html#case-study-common-bean"><i class="fa fa-check"></i><b>8.8.1</b> Case Study: Common Bean<span></span></a></li>
<li class="chapter" data-level="8.8.2" data-path="means-separation-and-data-presentation.html"><a href="means-separation-and-data-presentation.html#least-significant-difference-1"><i class="fa fa-check"></i><b>8.8.2</b> Least Significant Difference<span></span></a></li>
<li class="chapter" data-level="8.8.3" data-path="means-separation-and-data-presentation.html"><a href="means-separation-and-data-presentation.html#tukey-hsd"><i class="fa fa-check"></i><b>8.8.3</b> Tukey HSD<span></span></a></li>
<li class="chapter" data-level="8.8.4" data-path="means-separation-and-data-presentation.html"><a href="means-separation-and-data-presentation.html#practice-apple"><i class="fa fa-check"></i><b>8.8.4</b> Practice: Apple<span></span></a></li>
<li class="chapter" data-level="8.8.5" data-path="means-separation-and-data-presentation.html"><a href="means-separation-and-data-presentation.html#practice-wheat-treatment-with-mildew"><i class="fa fa-check"></i><b>8.8.5</b> Practice: Wheat Treatment with Mildew<span></span></a></li>
</ul></li>
<li class="chapter" data-level="8.9" data-path="means-separation-and-data-presentation.html"><a href="means-separation-and-data-presentation.html#exercise-linear-contrasts"><i class="fa fa-check"></i><b>8.9</b> Exercise: Linear Contrasts<span></span></a>
<ul>
<li class="chapter" data-level="8.9.1" data-path="means-separation-and-data-presentation.html"><a href="means-separation-and-data-presentation.html#case-study-winter-canola-cultivar-trial."><i class="fa fa-check"></i><b>8.9.1</b> Case Study: Winter Canola Cultivar Trial.<span></span></a></li>
<li class="chapter" data-level="8.9.2" data-path="means-separation-and-data-presentation.html"><a href="means-separation-and-data-presentation.html#running-the-contrast"><i class="fa fa-check"></i><b>8.9.2</b> Running the Contrast<span></span></a></li>
<li class="chapter" data-level="8.9.3" data-path="means-separation-and-data-presentation.html"><a href="means-separation-and-data-presentation.html#practice-corn-nitrogen-source-and-timing"><i class="fa fa-check"></i><b>8.9.3</b> Practice: Corn Nitrogen Source and Timing<span></span></a></li>
</ul></li>
<li class="chapter" data-level="8.10" data-path="means-separation-and-data-presentation.html"><a href="means-separation-and-data-presentation.html#exercise-means-tables"><i class="fa fa-check"></i><b>8.10</b> Exercise: Means Tables<span></span></a>
<ul>
<li class="chapter" data-level="8.10.1" data-path="means-separation-and-data-presentation.html"><a href="means-separation-and-data-presentation.html#case-study-corn-nitrogen-source-and-timing"><i class="fa fa-check"></i><b>8.10.1</b> Case Study: Corn Nitrogen Source and Timing<span></span></a></li>
<li class="chapter" data-level="8.10.2" data-path="means-separation-and-data-presentation.html"><a href="means-separation-and-data-presentation.html#practice-canola"><i class="fa fa-check"></i><b>8.10.2</b> Practice: Canola<span></span></a></li>
<li class="chapter" data-level="8.10.3" data-path="means-separation-and-data-presentation.html"><a href="means-separation-and-data-presentation.html#practice-broccoli-lsd"><i class="fa fa-check"></i><b>8.10.3</b> Practice: Broccoli LSD<span></span></a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="9" data-path="messy-and-missing-data.html"><a href="messy-and-missing-data.html"><i class="fa fa-check"></i><b>9</b> Messy and Missing Data<span></span></a>
<ul>
<li class="chapter" data-level="9.1" data-path="messy-and-missing-data.html"><a href="messy-and-missing-data.html#inspecting-data-for-normal-distributions"><i class="fa fa-check"></i><b>9.1</b> Inspecting data for Normal Distributions<span></span></a>
<ul>
<li class="chapter" data-level="9.1.1" data-path="messy-and-missing-data.html"><a href="messy-and-missing-data.html#histograms-1"><i class="fa fa-check"></i><b>9.1.1</b> Histograms<span></span></a></li>
<li class="chapter" data-level="9.1.2" data-path="messy-and-missing-data.html"><a href="messy-and-missing-data.html#rank-percentile-plots"><i class="fa fa-check"></i><b>9.1.2</b> Rank Percentile Plots<span></span></a></li>
<li class="chapter" data-level="9.1.3" data-path="messy-and-missing-data.html"><a href="messy-and-missing-data.html#box-plots"><i class="fa fa-check"></i><b>9.1.3</b> Box Plots<span></span></a></li>
</ul></li>
<li class="chapter" data-level="9.2" data-path="messy-and-missing-data.html"><a href="messy-and-missing-data.html#inspecting-data-for-equal-variances"><i class="fa fa-check"></i><b>9.2</b> Inspecting Data for Equal Variances<span></span></a>
<ul>
<li class="chapter" data-level="9.2.1" data-path="messy-and-missing-data.html"><a href="messy-and-missing-data.html#mean-variance-plot"><i class="fa fa-check"></i><b>9.2.1</b> Mean-Variance Plot<span></span></a></li>
<li class="chapter" data-level="9.2.2" data-path="messy-and-missing-data.html"><a href="messy-and-missing-data.html#homogeneity-of-variance-tests"><i class="fa fa-check"></i><b>9.2.2</b> Homogeneity of Variance Tests<span></span></a></li>
</ul></li>
<li class="chapter" data-level="9.3" data-path="messy-and-missing-data.html"><a href="messy-and-missing-data.html#dealing-with-messy-data"><i class="fa fa-check"></i><b>9.3</b> Dealing with Messy Data<span></span></a>
<ul>
<li class="chapter" data-level="9.3.1" data-path="messy-and-missing-data.html"><a href="messy-and-missing-data.html#outliers"><i class="fa fa-check"></i><b>9.3.1</b> Outliers<span></span></a></li>
<li class="chapter" data-level="9.3.2" data-path="messy-and-missing-data.html"><a href="messy-and-missing-data.html#non-normal-data-and-unequal-variances"><i class="fa fa-check"></i><b>9.3.2</b> Non-normal Data and Unequal Variances<span></span></a></li>
</ul></li>
<li class="chapter" data-level="9.4" data-path="messy-and-missing-data.html"><a href="messy-and-missing-data.html#dealing-with-missing-data"><i class="fa fa-check"></i><b>9.4</b> Dealing with Missing Data<span></span></a></li>
<li class="chapter" data-level="9.5" data-path="messy-and-missing-data.html"><a href="messy-and-missing-data.html#summary"><i class="fa fa-check"></i><b>9.5</b> Summary<span></span></a></li>
<li class="chapter" data-level="9.6" data-path="messy-and-missing-data.html"><a href="messy-and-missing-data.html#exercise-visual-data-inspection"><i class="fa fa-check"></i><b>9.6</b> Exercise: Visual Data Inspection<span></span></a>
<ul>
<li class="chapter" data-level="9.6.1" data-path="messy-and-missing-data.html"><a href="messy-and-missing-data.html#case-study-1-2"><i class="fa fa-check"></i><b>9.6.1</b> Case Study 1<span></span></a></li>
<li class="chapter" data-level="9.6.2" data-path="messy-and-missing-data.html"><a href="messy-and-missing-data.html#histogram"><i class="fa fa-check"></i><b>9.6.2</b> Histogram<span></span></a></li>
<li class="chapter" data-level="9.6.3" data-path="messy-and-missing-data.html"><a href="messy-and-missing-data.html#rank-percentile"><i class="fa fa-check"></i><b>9.6.3</b> Rank-Percentile<span></span></a></li>
<li class="chapter" data-level="9.6.4" data-path="messy-and-missing-data.html"><a href="messy-and-missing-data.html#case-study-2-2"><i class="fa fa-check"></i><b>9.6.4</b> Case Study 2<span></span></a></li>
<li class="chapter" data-level="9.6.5" data-path="messy-and-missing-data.html"><a href="messy-and-missing-data.html#box-plot"><i class="fa fa-check"></i><b>9.6.5</b> Box Plot<span></span></a></li>
<li class="chapter" data-level="9.6.6" data-path="messy-and-missing-data.html"><a href="messy-and-missing-data.html#practice-4"><i class="fa fa-check"></i><b>9.6.6</b> Practice<span></span></a></li>
</ul></li>
<li class="chapter" data-level="9.7" data-path="messy-and-missing-data.html"><a href="messy-and-missing-data.html#exercise-identifying-unequal-variances"><i class="fa fa-check"></i><b>9.7</b> Exercise: Identifying Unequal Variances”<span></span></a>
<ul>
<li class="chapter" data-level="9.7.1" data-path="messy-and-missing-data.html"><a href="messy-and-missing-data.html#case-study-grape-colapsis-in-seed-corn"><i class="fa fa-check"></i><b>9.7.1</b> Case Study: Grape Colapsis in Seed Corn<span></span></a></li>
<li class="chapter" data-level="9.7.2" data-path="messy-and-missing-data.html"><a href="messy-and-missing-data.html#practice-pigweed-height"><i class="fa fa-check"></i><b>9.7.2</b> Practice: Pigweed Height<span></span></a></li>
</ul></li>
<li class="chapter" data-level="9.8" data-path="messy-and-missing-data.html"><a href="messy-and-missing-data.html#exercise-transforming-and-analyzing-data"><i class="fa fa-check"></i><b>9.8</b> Exercise: Transforming and Analyzing Data<span></span></a>
<ul>
<li class="chapter" data-level="9.8.1" data-path="messy-and-missing-data.html"><a href="messy-and-missing-data.html#case-study-grape-colapsis-in-seed-corn-1"><i class="fa fa-check"></i><b>9.8.1</b> Case Study: Grape Colapsis in Seed Corn<span></span></a></li>
<li class="chapter" data-level="9.8.2" data-path="messy-and-missing-data.html"><a href="messy-and-missing-data.html#practice-pigweed-height-1"><i class="fa fa-check"></i><b>9.8.2</b> Practice: Pigweed Height<span></span></a></li>
<li class="chapter" data-level="9.8.3" data-path="messy-and-missing-data.html"><a href="messy-and-missing-data.html#practice-pigweed-biomass"><i class="fa fa-check"></i><b>9.8.3</b> Practice: Pigweed Biomass<span></span></a></li>
</ul></li>
<li class="chapter" data-level="9.9" data-path="messy-and-missing-data.html"><a href="messy-and-missing-data.html#exercise-imputing-missing-data"><i class="fa fa-check"></i><b>9.9</b> Exercise: Imputing Missing Data<span></span></a>
<ul>
<li class="chapter" data-level="9.9.1" data-path="messy-and-missing-data.html"><a href="messy-and-missing-data.html#case-study-wheat-genotype-trial"><i class="fa fa-check"></i><b>9.9.1</b> Case Study: Wheat Genotype Trial<span></span></a></li>
<li class="chapter" data-level="9.9.2" data-path="messy-and-missing-data.html"><a href="messy-and-missing-data.html#practice-1-1"><i class="fa fa-check"></i><b>9.9.2</b> Practice 1<span></span></a></li>
<li class="chapter" data-level="9.9.3" data-path="messy-and-missing-data.html"><a href="messy-and-missing-data.html#practice-2-1"><i class="fa fa-check"></i><b>9.9.3</b> Practice 2<span></span></a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="10" data-path="correlation-and-simple-regression.html"><a href="correlation-and-simple-regression.html"><i class="fa fa-check"></i><b>10</b> Correlation and Simple Regression<span></span></a>
<ul>
<li class="chapter" data-level="10.1" data-path="correlation-and-simple-regression.html"><a href="correlation-and-simple-regression.html#correlation"><i class="fa fa-check"></i><b>10.1</b> Correlation<span></span></a>
<ul>
<li class="chapter" data-level="10.1.1" data-path="correlation-and-simple-regression.html"><a href="correlation-and-simple-regression.html#case-study-cucumber"><i class="fa fa-check"></i><b>10.1.1</b> Case Study: Cucumber<span></span></a></li>
</ul></li>
<li class="chapter" data-level="10.2" data-path="correlation-and-simple-regression.html"><a href="correlation-and-simple-regression.html#correlation-1"><i class="fa fa-check"></i><b>10.2</b> Correlation<span></span></a>
<ul>
<li class="chapter" data-level="10.2.1" data-path="correlation-and-simple-regression.html"><a href="correlation-and-simple-regression.html#how-is-r-calcuated-optional-reading"><i class="fa fa-check"></i><b>10.2.1</b> How Is r Calcuated (optional reading)<span></span></a></li>
</ul></li>
<li class="chapter" data-level="10.3" data-path="correlation-and-simple-regression.html"><a href="correlation-and-simple-regression.html#regression"><i class="fa fa-check"></i><b>10.3</b> Regression<span></span></a>
<ul>
<li class="chapter" data-level="10.3.1" data-path="correlation-and-simple-regression.html"><a href="correlation-and-simple-regression.html#case-study-6"><i class="fa fa-check"></i><b>10.3.1</b> Case Study<span></span></a></li>
<li class="chapter" data-level="10.3.2" data-path="correlation-and-simple-regression.html"><a href="correlation-and-simple-regression.html#linear-equation"><i class="fa fa-check"></i><b>10.3.2</b> Linear Equation<span></span></a></li>
<li class="chapter" data-level="10.3.3" data-path="correlation-and-simple-regression.html"><a href="correlation-and-simple-regression.html#calculating-the-regression-equation"><i class="fa fa-check"></i><b>10.3.3</b> Calculating the Regression Equation<span></span></a></li>
<li class="chapter" data-level="10.3.4" data-path="correlation-and-simple-regression.html"><a href="correlation-and-simple-regression.html#least-squares-estimate"><i class="fa fa-check"></i><b>10.3.4</b> Least-Squares Estimate<span></span></a></li>
<li class="chapter" data-level="10.3.5" data-path="correlation-and-simple-regression.html"><a href="correlation-and-simple-regression.html#significance-of-coefficients"><i class="fa fa-check"></i><b>10.3.5</b> Significance of Coefficients<span></span></a></li>
<li class="chapter" data-level="10.3.6" data-path="correlation-and-simple-regression.html"><a href="correlation-and-simple-regression.html#analysis-of-variance-6"><i class="fa fa-check"></i><b>10.3.6</b> Analysis of Variance<span></span></a></li>
<li class="chapter" data-level="10.3.7" data-path="correlation-and-simple-regression.html"><a href="correlation-and-simple-regression.html#measuring-model-fit-with-r-square"><i class="fa fa-check"></i><b>10.3.7</b> Measuring Model Fit with R-Square<span></span></a></li>
<li class="chapter" data-level="10.3.8" data-path="correlation-and-simple-regression.html"><a href="correlation-and-simple-regression.html#checking-whether-the-linear-model-is-appropriate"><i class="fa fa-check"></i><b>10.3.8</b> Checking whether the Linear Model is Appropriate<span></span></a></li>
</ul></li>
<li class="chapter" data-level="10.4" data-path="correlation-and-simple-regression.html"><a href="correlation-and-simple-regression.html#extrapolation"><i class="fa fa-check"></i><b>10.4</b> Extrapolation<span></span></a></li>
<li class="chapter" data-level="10.5" data-path="correlation-and-simple-regression.html"><a href="correlation-and-simple-regression.html#exercise-sccatterplots-and-regression"><i class="fa fa-check"></i><b>10.5</b> Exercise: Sccatterplots and Regression<span></span></a>
<ul>
<li class="chapter" data-level="10.5.1" data-path="correlation-and-simple-regression.html"><a href="correlation-and-simple-regression.html#case-study-corn-data"><i class="fa fa-check"></i><b>10.5.1</b> Case Study: Corn Data<span></span></a></li>
<li class="chapter" data-level="10.5.2" data-path="correlation-and-simple-regression.html"><a href="correlation-and-simple-regression.html#practice-1-2"><i class="fa fa-check"></i><b>10.5.2</b> Practice 1<span></span></a></li>
<li class="chapter" data-level="10.5.3" data-path="correlation-and-simple-regression.html"><a href="correlation-and-simple-regression.html#practice-2-2"><i class="fa fa-check"></i><b>10.5.3</b> Practice 2<span></span></a></li>
<li class="chapter" data-level="10.5.4" data-path="correlation-and-simple-regression.html"><a href="correlation-and-simple-regression.html#practice-3-1"><i class="fa fa-check"></i><b>10.5.4</b> Practice 3<span></span></a></li>
<li class="chapter" data-level="10.5.5" data-path="correlation-and-simple-regression.html"><a href="correlation-and-simple-regression.html#practice-4-1"><i class="fa fa-check"></i><b>10.5.5</b> Practice 4<span></span></a></li>
</ul></li>
<li class="chapter" data-level="10.6" data-path="correlation-and-simple-regression.html"><a href="correlation-and-simple-regression.html#exercise-simple-linear-regression"><i class="fa fa-check"></i><b>10.6</b> Exercise: Simple Linear Regression<span></span></a>
<ul>
<li class="chapter" data-level="10.6.1" data-path="correlation-and-simple-regression.html"><a href="correlation-and-simple-regression.html#case-study-corn-allometry"><i class="fa fa-check"></i><b>10.6.1</b> Case Study: Corn Allometry<span></span></a></li>
<li class="chapter" data-level="10.6.2" data-path="correlation-and-simple-regression.html"><a href="correlation-and-simple-regression.html#fitting-the-regression-model"><i class="fa fa-check"></i><b>10.6.2</b> Fitting the regression model<span></span></a></li>
<li class="chapter" data-level="10.6.3" data-path="correlation-and-simple-regression.html"><a href="correlation-and-simple-regression.html#examining-the-residuals"><i class="fa fa-check"></i><b>10.6.3</b> Examining the residuals<span></span></a></li>
<li class="chapter" data-level="10.6.4" data-path="correlation-and-simple-regression.html"><a href="correlation-and-simple-regression.html#viewing-the-model-coefficients"><i class="fa fa-check"></i><b>10.6.4</b> Viewing the Model Coefficients<span></span></a></li>
<li class="chapter" data-level="10.6.5" data-path="correlation-and-simple-regression.html"><a href="correlation-and-simple-regression.html#more-measures-of-model-fit"><i class="fa fa-check"></i><b>10.6.5</b> More measures of model fit<span></span></a></li>
<li class="chapter" data-level="10.6.6" data-path="correlation-and-simple-regression.html"><a href="correlation-and-simple-regression.html#practice-1-3"><i class="fa fa-check"></i><b>10.6.6</b> Practice 1<span></span></a></li>
<li class="chapter" data-level="10.6.7" data-path="correlation-and-simple-regression.html"><a href="correlation-and-simple-regression.html#practice-2-3"><i class="fa fa-check"></i><b>10.6.7</b> Practice 2<span></span></a></li>
</ul></li>
<li class="chapter" data-level="10.7" data-path="correlation-and-simple-regression.html"><a href="correlation-and-simple-regression.html#exercise-making-predictions-from-regression-models."><i class="fa fa-check"></i><b>10.7</b> Exercise: Making Predictions from Regression Models.<span></span></a>
<ul>
<li class="chapter" data-level="10.7.1" data-path="correlation-and-simple-regression.html"><a href="correlation-and-simple-regression.html#case-study-7"><i class="fa fa-check"></i><b>10.7.1</b> Case Study<span></span></a></li>
<li class="chapter" data-level="10.7.2" data-path="correlation-and-simple-regression.html"><a href="correlation-and-simple-regression.html#creating-and-testing-our-model"><i class="fa fa-check"></i><b>10.7.2</b> Creating and Testing our Model<span></span></a></li>
<li class="chapter" data-level="10.7.3" data-path="correlation-and-simple-regression.html"><a href="correlation-and-simple-regression.html#visualizing-the-regression-model"><i class="fa fa-check"></i><b>10.7.3</b> Visualizing the Regression Model<span></span></a></li>
<li class="chapter" data-level="10.7.4" data-path="correlation-and-simple-regression.html"><a href="correlation-and-simple-regression.html#making-predictions-with-the-regression-model"><i class="fa fa-check"></i><b>10.7.4</b> Making Predictions with the Regression Model<span></span></a></li>
<li class="chapter" data-level="10.7.5" data-path="correlation-and-simple-regression.html"><a href="correlation-and-simple-regression.html#practice-1-4"><i class="fa fa-check"></i><b>10.7.5</b> Practice 1<span></span></a></li>
<li class="chapter" data-level="10.7.6" data-path="correlation-and-simple-regression.html"><a href="correlation-and-simple-regression.html#practice-2-4"><i class="fa fa-check"></i><b>10.7.6</b> Practice 2<span></span></a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="11" data-path="nonlinear-relationships-and-multiple-linear-regression.html"><a href="nonlinear-relationships-and-multiple-linear-regression.html"><i class="fa fa-check"></i><b>11</b> Nonlinear Relationships and Multiple Linear Regression<span></span></a>
<ul>
<li class="chapter" data-level="11.1" data-path="nonlinear-relationships-and-multiple-linear-regression.html"><a href="nonlinear-relationships-and-multiple-linear-regression.html#multiplie-linear-regression"><i class="fa fa-check"></i><b>11.1</b> Multiplie Linear Regression<span></span></a>
<ul>
<li class="chapter" data-level="11.1.1" data-path="nonlinear-relationships-and-multiple-linear-regression.html"><a href="nonlinear-relationships-and-multiple-linear-regression.html#case-study-modelling-yield-by-county"><i class="fa fa-check"></i><b>11.1.1</b> Case Study: Modelling Yield by County<span></span></a></li>
<li class="chapter" data-level="11.1.2" data-path="nonlinear-relationships-and-multiple-linear-regression.html"><a href="nonlinear-relationships-and-multiple-linear-regression.html#beware-of-bloated-models"><i class="fa fa-check"></i><b>11.1.2</b> Beware of Bloated Models<span></span></a></li>
<li class="chapter" data-level="11.1.3" data-path="nonlinear-relationships-and-multiple-linear-regression.html"><a href="nonlinear-relationships-and-multiple-linear-regression.html#methods-for-avoiding-bloated-models"><i class="fa fa-check"></i><b>11.1.3</b> Methods for Avoiding Bloated Models<span></span></a></li>
<li class="chapter" data-level="11.1.4" data-path="nonlinear-relationships-and-multiple-linear-regression.html"><a href="nonlinear-relationships-and-multiple-linear-regression.html#tunning-and-comparing-models"><i class="fa fa-check"></i><b>11.1.4</b> Tunning and Comparing Models<span></span></a></li>
</ul></li>
<li class="chapter" data-level="11.2" data-path="nonlinear-relationships-and-multiple-linear-regression.html"><a href="nonlinear-relationships-and-multiple-linear-regression.html#nonlinear-relationships"><i class="fa fa-check"></i><b>11.2</b> Nonlinear Relationships<span></span></a>
<ul>
<li class="chapter" data-level="11.2.1" data-path="nonlinear-relationships-and-multiple-linear-regression.html"><a href="nonlinear-relationships-and-multiple-linear-regression.html#fitting-nonlinear-responses-with-linear-regression"><i class="fa fa-check"></i><b>11.2.1</b> Fitting Nonlinear Responses with Linear Regression<span></span></a></li>
<li class="chapter" data-level="11.2.2" data-path="nonlinear-relationships-and-multiple-linear-regression.html"><a href="nonlinear-relationships-and-multiple-linear-regression.html#fitting-nonlinear-responses-with-nonlinear-regression"><i class="fa fa-check"></i><b>11.2.2</b> Fitting Nonlinear Responses with Nonlinear Regression<span></span></a></li>
</ul></li>
<li class="chapter" data-level="11.3" data-path="nonlinear-relationships-and-multiple-linear-regression.html"><a href="nonlinear-relationships-and-multiple-linear-regression.html#summary-1"><i class="fa fa-check"></i><b>11.3</b> Summary<span></span></a></li>
<li class="chapter" data-level="11.4" data-path="nonlinear-relationships-and-multiple-linear-regression.html"><a href="nonlinear-relationships-and-multiple-linear-regression.html#exercise-multiple-linear-regression"><i class="fa fa-check"></i><b>11.4</b> Exercise: Multiple Linear Regression<span></span></a>
<ul>
<li class="chapter" data-level="11.4.1" data-path="nonlinear-relationships-and-multiple-linear-regression.html"><a href="nonlinear-relationships-and-multiple-linear-regression.html#case-study-country-corn-yields"><i class="fa fa-check"></i><b>11.4.1</b> Case Study: Country Corn Yields<span></span></a></li>
<li class="chapter" data-level="11.4.2" data-path="nonlinear-relationships-and-multiple-linear-regression.html"><a href="nonlinear-relationships-and-multiple-linear-regression.html#fitting-the-model"><i class="fa fa-check"></i><b>11.4.2</b> Fitting the Model<span></span></a></li>
<li class="chapter" data-level="11.4.3" data-path="nonlinear-relationships-and-multiple-linear-regression.html"><a href="nonlinear-relationships-and-multiple-linear-regression.html#inspecting-the-model"><i class="fa fa-check"></i><b>11.4.3</b> Inspecting the Model<span></span></a></li>
<li class="chapter" data-level="11.4.4" data-path="nonlinear-relationships-and-multiple-linear-regression.html"><a href="nonlinear-relationships-and-multiple-linear-regression.html#practice-thompson-corn-data"><i class="fa fa-check"></i><b>11.4.4</b> Practice: Thompson Corn Data<span></span></a></li>
</ul></li>
<li class="chapter" data-level="11.5" data-path="nonlinear-relationships-and-multiple-linear-regression.html"><a href="nonlinear-relationships-and-multiple-linear-regression.html#exercise-avoiding-bloated-models"><i class="fa fa-check"></i><b>11.5</b> Exercise: Avoiding Bloated Models<span></span></a>
<ul>
<li class="chapter" data-level="11.5.1" data-path="nonlinear-relationships-and-multiple-linear-regression.html"><a href="nonlinear-relationships-and-multiple-linear-regression.html#case-study-8"><i class="fa fa-check"></i><b>11.5.1</b> Case Study<span></span></a></li>
<li class="chapter" data-level="11.5.2" data-path="nonlinear-relationships-and-multiple-linear-regression.html"><a href="nonlinear-relationships-and-multiple-linear-regression.html#covariance-matrix"><i class="fa fa-check"></i><b>11.5.2</b> Covariance Matrix<span></span></a></li>
<li class="chapter" data-level="11.5.3" data-path="nonlinear-relationships-and-multiple-linear-regression.html"><a href="nonlinear-relationships-and-multiple-linear-regression.html#partial-correlation-1"><i class="fa fa-check"></i><b>11.5.3</b> Partial Correlation<span></span></a></li>
<li class="chapter" data-level="11.5.4" data-path="nonlinear-relationships-and-multiple-linear-regression.html"><a href="nonlinear-relationships-and-multiple-linear-regression.html#cross-validation-1"><i class="fa fa-check"></i><b>11.5.4</b> Cross-Validation<span></span></a></li>
<li class="chapter" data-level="11.5.5" data-path="nonlinear-relationships-and-multiple-linear-regression.html"><a href="nonlinear-relationships-and-multiple-linear-regression.html#practice-thompson-corn-data-1"><i class="fa fa-check"></i><b>11.5.5</b> Practice: Thompson Corn Data<span></span></a></li>
</ul></li>
<li class="chapter" data-level="11.6" data-path="nonlinear-relationships-and-multiple-linear-regression.html"><a href="nonlinear-relationships-and-multiple-linear-regression.html#exercise-tuning-the-model"><i class="fa fa-check"></i><b>11.6</b> Exercise: Tuning the Model<span></span></a>
<ul>
<li class="chapter" data-level="11.6.1" data-path="nonlinear-relationships-and-multiple-linear-regression.html"><a href="nonlinear-relationships-and-multiple-linear-regression.html#case-study-9"><i class="fa fa-check"></i><b>11.6.1</b> Case Study<span></span></a></li>
<li class="chapter" data-level="11.6.2" data-path="nonlinear-relationships-and-multiple-linear-regression.html"><a href="nonlinear-relationships-and-multiple-linear-regression.html#tuning-the-model"><i class="fa fa-check"></i><b>11.6.2</b> Tuning the Model<span></span></a></li>
<li class="chapter" data-level="11.6.3" data-path="nonlinear-relationships-and-multiple-linear-regression.html"><a href="nonlinear-relationships-and-multiple-linear-regression.html#practice-5"><i class="fa fa-check"></i><b>11.6.3</b> Practice<span></span></a></li>
<li class="chapter" data-level="11.6.4" data-path="nonlinear-relationships-and-multiple-linear-regression.html"><a href="nonlinear-relationships-and-multiple-linear-regression.html#practice-6"><i class="fa fa-check"></i><b>11.6.4</b> Practice<span></span></a></li>
</ul></li>
<li class="chapter" data-level="11.7" data-path="nonlinear-relationships-and-multiple-linear-regression.html"><a href="nonlinear-relationships-and-multiple-linear-regression.html#exercise-nonlinear-regression"><i class="fa fa-check"></i><b>11.7</b> Exercise: Nonlinear Regression<span></span></a>
<ul>
<li class="chapter" data-level="11.7.1" data-path="nonlinear-relationships-and-multiple-linear-regression.html"><a href="nonlinear-relationships-and-multiple-linear-regression.html#case-study-1-monomolecular-data"><i class="fa fa-check"></i><b>11.7.1</b> Case Study 1: Monomolecular Data<span></span></a></li>
<li class="chapter" data-level="11.7.2" data-path="nonlinear-relationships-and-multiple-linear-regression.html"><a href="nonlinear-relationships-and-multiple-linear-regression.html#case-study-2-logistic-data"><i class="fa fa-check"></i><b>11.7.2</b> Case Study 2: Logistic Data<span></span></a></li>
<li class="chapter" data-level="11.7.3" data-path="nonlinear-relationships-and-multiple-linear-regression.html"><a href="nonlinear-relationships-and-multiple-linear-regression.html#practice-1-5"><i class="fa fa-check"></i><b>11.7.3</b> Practice 1<span></span></a></li>
<li class="chapter" data-level="11.7.4" data-path="nonlinear-relationships-and-multiple-linear-regression.html"><a href="nonlinear-relationships-and-multiple-linear-regression.html#practice-2-5"><i class="fa fa-check"></i><b>11.7.4</b> Practice 2<span></span></a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="12" data-path="spatial-statistics.html"><a href="spatial-statistics.html"><i class="fa fa-check"></i><b>12</b> Spatial Statistics<span></span></a>
<ul>
<li class="chapter" data-level="12.1" data-path="spatial-statistics.html"><a href="spatial-statistics.html#projection-general"><i class="fa fa-check"></i><b>12.1</b> Projection (General)<span></span></a>
<ul>
<li class="chapter" data-level="12.1.1" data-path="spatial-statistics.html"><a href="spatial-statistics.html#wgs-84-epsg-4236"><i class="fa fa-check"></i><b>12.1.1</b> WGS 84 (EPSG: 4236)<span></span></a></li>
<li class="chapter" data-level="12.1.2" data-path="spatial-statistics.html"><a href="spatial-statistics.html#mercator-epsg-3857"><i class="fa fa-check"></i><b>12.1.2</b> Mercator (EPSG: 3857)<span></span></a></li>
<li class="chapter" data-level="12.1.3" data-path="spatial-statistics.html"><a href="spatial-statistics.html#us-national-atlas-equal-area-epsg-2163"><i class="fa fa-check"></i><b>12.1.3</b> US National Atlas Equal Area (EPSG: 2163)<span></span></a></li>
<li class="chapter" data-level="12.1.4" data-path="spatial-statistics.html"><a href="spatial-statistics.html#utm-zone-11n-epsg-2955"><i class="fa fa-check"></i><b>12.1.4</b> UTM Zone 11N (EPSG: 2955)<span></span></a></li>
<li class="chapter" data-level="12.1.5" data-path="spatial-statistics.html"><a href="spatial-statistics.html#projection-summary"><i class="fa fa-check"></i><b>12.1.5</b> Projection Summary<span></span></a></li>
</ul></li>
<li class="chapter" data-level="12.2" data-path="spatial-statistics.html"><a href="spatial-statistics.html#shape-files"><i class="fa fa-check"></i><b>12.2</b> Shape Files<span></span></a>
<ul>
<li class="chapter" data-level="12.2.1" data-path="spatial-statistics.html"><a href="spatial-statistics.html#case-study-soybean-yield-in-iowa"><i class="fa fa-check"></i><b>12.2.1</b> Case Study: Soybean Yield in Iowa<span></span></a></li>
<li class="chapter" data-level="12.2.2" data-path="spatial-statistics.html"><a href="spatial-statistics.html#ssurgo"><i class="fa fa-check"></i><b>12.2.2</b> SSURGO<span></span></a></li>
<li class="chapter" data-level="12.2.3" data-path="spatial-statistics.html"><a href="spatial-statistics.html#operations-with-shapes"><i class="fa fa-check"></i><b>12.2.3</b> Operations with Shapes<span></span></a></li>
</ul></li>
<li class="chapter" data-level="12.3" data-path="spatial-statistics.html"><a href="spatial-statistics.html#rasters"><i class="fa fa-check"></i><b>12.3</b> Rasters<span></span></a>
<ul>
<li class="chapter" data-level="12.3.1" data-path="spatial-statistics.html"><a href="spatial-statistics.html#interpolation"><i class="fa fa-check"></i><b>12.3.1</b> Interpolation<span></span></a></li>
<li class="chapter" data-level="12.3.2" data-path="spatial-statistics.html"><a href="spatial-statistics.html#kriging"><i class="fa fa-check"></i><b>12.3.2</b> Kriging<span></span></a></li>
<li class="chapter" data-level="12.3.3" data-path="spatial-statistics.html"><a href="spatial-statistics.html#operations-on-kriged-data"><i class="fa fa-check"></i><b>12.3.3</b> Operations on Kriged Data<span></span></a></li>
</ul></li>
<li class="chapter" data-level="12.4" data-path="spatial-statistics.html"><a href="spatial-statistics.html#exercise-shape-files"><i class="fa fa-check"></i><b>12.4</b> Exercise: Shape Files<span></span></a>
<ul>
<li class="chapter" data-level="12.4.1" data-path="spatial-statistics.html"><a href="spatial-statistics.html#case-study-10"><i class="fa fa-check"></i><b>12.4.1</b> Case Study<span></span></a></li>
<li class="chapter" data-level="12.4.2" data-path="spatial-statistics.html"><a href="spatial-statistics.html#practice-7"><i class="fa fa-check"></i><b>12.4.2</b> Practice<span></span></a></li>
</ul></li>
<li class="chapter" data-level="12.5" data-path="spatial-statistics.html"><a href="spatial-statistics.html#exercise-ssurgo"><i class="fa fa-check"></i><b>12.5</b> Exercise: SSURGO<span></span></a></li>
<li class="chapter" data-level="12.6" data-path="spatial-statistics.html"><a href="spatial-statistics.html#exercise-rasters"><i class="fa fa-check"></i><b>12.6</b> Exercise: Rasters<span></span></a>
<ul>
<li class="chapter" data-level="12.6.1" data-path="spatial-statistics.html"><a href="spatial-statistics.html#case-study-soil-test-data"><i class="fa fa-check"></i><b>12.6.1</b> Case Study: Soil Test Data<span></span></a></li>
<li class="chapter" data-level="12.6.2" data-path="spatial-statistics.html"><a href="spatial-statistics.html#practice-1-6"><i class="fa fa-check"></i><b>12.6.2</b> Practice 1<span></span></a></li>
<li class="chapter" data-level="12.6.3" data-path="spatial-statistics.html"><a href="spatial-statistics.html#practice-2-6"><i class="fa fa-check"></i><b>12.6.3</b> Practice 2<span></span></a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="13" data-path="machine-learning.html"><a href="machine-learning.html"><i class="fa fa-check"></i><b>13</b> Machine Learning<span></span></a>
<ul>
<li class="chapter" data-level="13.1" data-path="machine-learning.html"><a href="machine-learning.html#machine-learning-1"><i class="fa fa-check"></i><b>13.1</b> Machine Learning<span></span></a></li>
<li class="chapter" data-level="13.2" data-path="machine-learning.html"><a href="machine-learning.html#cluster-analyses"><i class="fa fa-check"></i><b>13.2</b> Cluster Analyses<span></span></a>
<ul>
<li class="chapter" data-level="13.2.1" data-path="machine-learning.html"><a href="machine-learning.html#case-study-grouping-midwestern-environments"><i class="fa fa-check"></i><b>13.2.1</b> Case Study: Grouping Midwestern Environments<span></span></a></li>
<li class="chapter" data-level="13.2.2" data-path="machine-learning.html"><a href="machine-learning.html#scaling"><i class="fa fa-check"></i><b>13.2.2</b> Scaling<span></span></a></li>
<li class="chapter" data-level="13.2.3" data-path="machine-learning.html"><a href="machine-learning.html#clustering-animation"><i class="fa fa-check"></i><b>13.2.3</b> Clustering Animation<span></span></a></li>
<li class="chapter" data-level="13.2.4" data-path="machine-learning.html"><a href="machine-learning.html#county-cluster-analysis"><i class="fa fa-check"></i><b>13.2.4</b> County Cluster Analysis<span></span></a></li>
</ul></li>
<li class="chapter" data-level="13.3" data-path="machine-learning.html"><a href="machine-learning.html#k-nearest-neighbors"><i class="fa fa-check"></i><b>13.3</b> k-Nearest-Neighbors<span></span></a>
<ul>
<li class="chapter" data-level="13.3.1" data-path="machine-learning.html"><a href="machine-learning.html#case-study-guessing-county-yields-based-on-environmental-similarity"><i class="fa fa-check"></i><b>13.3.1</b> Case Study: Guessing County Yields based on Environmental Similarity<span></span></a></li>
<li class="chapter" data-level="13.3.2" data-path="machine-learning.html"><a href="machine-learning.html#scaling-1"><i class="fa fa-check"></i><b>13.3.2</b> Scaling<span></span></a></li>
<li class="chapter" data-level="13.3.3" data-path="machine-learning.html"><a href="machine-learning.html#k-nearest-neighbor-animation"><i class="fa fa-check"></i><b>13.3.3</b> k-Nearest-Neighbor Animation<span></span></a></li>
<li class="chapter" data-level="13.3.4" data-path="machine-learning.html"><a href="machine-learning.html#choosing-k"><i class="fa fa-check"></i><b>13.3.4</b> Choosing <span class="math inline">\(k\)</span><span></span></a></li>
<li class="chapter" data-level="13.3.5" data-path="machine-learning.html"><a href="machine-learning.html#model-cross-validation"><i class="fa fa-check"></i><b>13.3.5</b> Model Cross-Validation<span></span></a></li>
<li class="chapter" data-level="13.3.6" data-path="machine-learning.html"><a href="machine-learning.html#yield-prediction-with-nearest-neighbor-analysis"><i class="fa fa-check"></i><b>13.3.6</b> Yield Prediction with Nearest Neighbor Analysis<span></span></a></li>
</ul></li>
<li class="chapter" data-level="13.4" data-path="machine-learning.html"><a href="machine-learning.html#classification-trees"><i class="fa fa-check"></i><b>13.4</b> Classification Trees<span></span></a>
<ul>
<li class="chapter" data-level="13.4.1" data-path="machine-learning.html"><a href="machine-learning.html#features"><i class="fa fa-check"></i><b>13.4.1</b> Features<span></span></a></li>
<li class="chapter" data-level="13.4.2" data-path="machine-learning.html"><a href="machine-learning.html#quantitative-categorical-data"><i class="fa fa-check"></i><b>13.4.2</b> Quantitative (Categorical) Data<span></span></a></li>
<li class="chapter" data-level="13.4.3" data-path="machine-learning.html"><a href="machine-learning.html#quanatitiative-continuous-data"><i class="fa fa-check"></i><b>13.4.3</b> Quanatitiative (Continuous) Data<span></span></a></li>
<li class="chapter" data-level="13.4.4" data-path="machine-learning.html"><a href="machine-learning.html#overfitting-1"><i class="fa fa-check"></i><b>13.4.4</b> Overfitting<span></span></a></li>
<li class="chapter" data-level="13.4.5" data-path="machine-learning.html"><a href="machine-learning.html#cross-validation-2"><i class="fa fa-check"></i><b>13.4.5</b> Cross Validation<span></span></a></li>
<li class="chapter" data-level="13.4.6" data-path="machine-learning.html"><a href="machine-learning.html#random-forest"><i class="fa fa-check"></i><b>13.4.6</b> Random Forest<span></span></a></li>
<li class="chapter" data-level="13.4.7" data-path="machine-learning.html"><a href="machine-learning.html#feature-importance"><i class="fa fa-check"></i><b>13.4.7</b> Feature Importance<span></span></a></li>
</ul></li>
<li class="chapter" data-level="13.5" data-path="machine-learning.html"><a href="machine-learning.html#summary-2"><i class="fa fa-check"></i><b>13.5</b> Summary<span></span></a></li>
</ul></li>
<li class="chapter" data-level="14" data-path="putting-it-all-together.html"><a href="putting-it-all-together.html"><i class="fa fa-check"></i><b>14</b> Putting it all Together<span></span></a>
<ul>
<li class="chapter" data-level="14.1" data-path="putting-it-all-together.html"><a href="putting-it-all-together.html#scenario-1-yield-map-population-summary-and-z-distribution"><i class="fa fa-check"></i><b>14.1</b> Scenario 1: Yield Map (Population Summary and Z-Distribution)<span></span></a></li>
<li class="chapter" data-level="14.2" data-path="putting-it-all-together.html"><a href="putting-it-all-together.html#scenario-2-yield-estimate-sampling-t-distribution"><i class="fa fa-check"></i><b>14.2</b> Scenario 2: Yield Estimate (Sampling t-Distribution)<span></span></a></li>
<li class="chapter" data-level="14.3" data-path="putting-it-all-together.html"><a href="putting-it-all-together.html#scenario-3-side-by-side-t-test"><i class="fa fa-check"></i><b>14.3</b> Scenario 3: Side-By-Side (t-Test)<span></span></a></li>
<li class="chapter" data-level="14.4" data-path="putting-it-all-together.html"><a href="putting-it-all-together.html#scenario-4-fungicide-trial-anova-crd-or-rcbd"><i class="fa fa-check"></i><b>14.4</b> Scenario 4: Fungicide Trial (ANOVA CRD or RCBD)<span></span></a></li>
<li class="chapter" data-level="14.5" data-path="putting-it-all-together.html"><a href="putting-it-all-together.html#scenario-5-hybrid-response-to-fungicide-trial-anova-factorial-or-split-plot"><i class="fa fa-check"></i><b>14.5</b> Scenario 5: Hybrid Response to Fungicide Trial (ANOVA Factorial or Split Plot)<span></span></a></li>
<li class="chapter" data-level="14.6" data-path="putting-it-all-together.html"><a href="putting-it-all-together.html#scenario-6-foliar-rate-response-trial-linear-or-non-linear-regression"><i class="fa fa-check"></i><b>14.6</b> Scenario 6: Foliar Rate-Response Trial (Linear or Non-Linear Regression)<span></span></a></li>
<li class="chapter" data-level="14.7" data-path="putting-it-all-together.html"><a href="putting-it-all-together.html#scenario-7-application-map-shapefiles-and-rasters"><i class="fa fa-check"></i><b>14.7</b> Scenario 7: Application Map (Shapefiles and Rasters)<span></span></a></li>
<li class="chapter" data-level="14.8" data-path="putting-it-all-together.html"><a href="putting-it-all-together.html#scenario-8-yield-prediction-multiple-linear-regression-and-other-predictive-models"><i class="fa fa-check"></i><b>14.8</b> Scenario 8: Yield Prediction (Multiple Linear Regression and other Predictive Models)<span></span></a></li>
<li class="chapter" data-level="14.9" data-path="putting-it-all-together.html"><a href="putting-it-all-together.html#summary-3"><i class="fa fa-check"></i><b>14.9</b> Summary<span></span></a></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Data Science for Agricultural Professionals</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="machine-learning" class="section level1 hasAnchor" number="13">
<h1><span class="header-section-number">Chapter 13</span> Machine Learning<a href="machine-learning.html#machine-learning" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p>Machine Learning is an advanced data science topic that I have struggled over whether to include in this text. On the one hand, it is not generally considered part of classical statistics. In addition, unless your plans include becoming a data analyst, you are unlikely to run these analyses on your own. On the other hand, machine learning is playing an increasing role in agricultural (and all areas of) data. It is a topic in which I have been immersed during the past couple of years.</p>
<p>Most importantly, if you are using tools to select hybrids, adjust nitrogen rates, or predict yield, those tools likely incorporate aspects of machine learning. In your daily lives, machine learning determines what advertisements you see, your social media feed, even the results from your search engines. This lesson emphasizes literacy about machine learning – there are no coding exercises included.</p>
<div id="machine-learning-1" class="section level2 hasAnchor" number="13.1">
<h2><span class="header-section-number">13.1</span> Machine Learning<a href="machine-learning.html#machine-learning-1" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>To start with, why is it called “machine learning”? It sounds like statistics-meets-steampunk. To answer this, think about a machine that learns. In other words: a robot. In machine learning, we use complex algoritms that, in theory, could be used by artificial intelligence to learn about their environment. There is a strong predictive component to this: the machine could learn to predict the outcomes of future events by building models based on data it has already obtained. Hello, Skynet.</p>
<p>This sword, however, is also a plowshare. Machine learning can help us understand how many variables can simultaneously predict outcomes, in agriculture, medicine, and climate. Furthermore, machine learning takes a different approach to models than the methods we have learned earlier. In previous units, we have learned testing methods that were <em>parameteric</em>. We defined a linear model, then used our tests (which were all variations of regression) to parameterize it. Parameteric =&gt; parameter.</p>
<p>We’ve also learned that parametric methods have their challenges. One challenge is non-normal or skewed data. Another is heterogeneity of variances. These challenges required us to transform the data, a step which can generate confusion in analyzing and summarizing data. We were also warned when working with multiple linear regression models to beware of multicollinearity (correlations between independent variables) and heteroscedasticity (unequal variances).</p>
<p>Machine learning uses <em>nonparametric</em> tests. As the name suggests, these do not use parameters. Instead of regression models, machine learning tools use logical tests (for example, does an observed value fall in a range) to “decide” what value to predict. They measure the “similarity” between two locations to decide whether an observation in one location is likely to occur in the other. They figure out how to group similar observations into groups or “clusters”.</p>
<p>In this unit, we will study three examples of machine learning. First, we will see how cluster analysis might be used to divide a sales territory into multiple regions, based on environment. Second, we will learn how to use <em>nearest-neighbor</em> analysis to predict yield for a given county, given its similarity to other counties. Finally, we will learn how <em>classification trees</em> can be used to explain yield response to multiple enviornmental factors.</p>
<p>This unit will be more focused on what I call “data literacy” than execution. I want you to be aware of these three tools, which are being used around you in the agronomy world. I want you to be able to speak of them, to ask questions, if engaged. I will resist going deep into the weeds with them, however, because they are often best suited to hundreds (ok) or many thousands (even better) of data points. It is unlikely you will use them in your Creative Component, or even your day-to-day research.</p>
<p>That said, just a few years ago, neither did I. Awareness and curiosity in data science are always good – there is always a more powerful way to address our questions.</p>
</div>
<div id="cluster-analyses" class="section level2 hasAnchor" number="13.2">
<h2><span class="header-section-number">13.2</span> Cluster Analyses<a href="machine-learning.html#cluster-analyses" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Cluster analysis takes a set of individuals (sometimes called examples in this analyses) and divides it into a set number of groups, based on the similarities between the individuals. These groups are defined such that the individuals within a group are very similar to each other, but very different from individuals in other groups.</p>
<p>By itself, clustering doesn’t answer any questions about the values of the individuals in our data set. Unlike a classification tree or a k-Nearest Neighbor analysis, it cannot be used to interpolate values between data points, or predict the value of a future observation.</p>
<p>To further illustrate cluster analysis, it is described in the data science community as “unsupervised learning.” This means that we do not begin with a variable of interest, nor a hypothesis about how changes in one variable relate to changes in another.</p>
<p>Why would we want to cluster data? The reason is that sometimes, when we are dealing with many variables, our analyses may become more intuitive if there are ways to break data down into groups. Clustering is a way of reducing or categorizing our data so it becomes less-overwhelming for us to deal with.</p>
<div id="case-study-grouping-midwestern-environments" class="section level3 hasAnchor" number="13.2.1">
<h3><span class="header-section-number">13.2.1</span> Case Study: Grouping Midwestern Environments<a href="machine-learning.html#case-study-grouping-midwestern-environments" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>For example, anytime we are working with environmental data, we have potentially enormous datasets. Rather than try juggle the meanings of all these quantitative variables in our heads, we may want to desrcibe the data as a series of groups. For example, we might say that a county in South Dakota is cold and dry, a county in northern Illinois is moderate temperature and moderate wetness, and a farm in western Ohio is warm and rainy. Great, we now have three groups. But how would other counties in the midwest sort into these three categories?</p>
<p>For an agronomic testing program, this is a very important question. If the intent is to address the individual needs of three different environments, we need to define those regions so that a sufficient, but not excessive, number of research locations can be designated within each environment.</p>
<p>Lets start out with dataset of county environments. The county environments are mostly soil data, which are not expected to change (outside of human influence) in the short term. Two data are climatic: growing degree days (gdd) and precipitation (ppt). Their values in this dataset reflect a 20-year mean.</p>
<p>Below I have plotted the mean annual precipitation (in millimeters). We see that precipitation is lowest in the Dakotas and generally increases as we move south and east.</p>
<div class="sourceCode" id="cb959"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb959-1"><a href="machine-learning.html#cb959-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(readr)</span>
<span id="cb959-2"><a href="machine-learning.html#cb959-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(tidyverse)</span></code></pre></div>
<pre><code>## -- Attaching packages --------------------------------------- tidyverse 1.3.1 --</code></pre>
<pre><code>## v ggplot2 3.3.5     v dplyr   1.0.8
## v tibble  3.1.6     v stringr 1.4.0
## v tidyr   1.2.0     v forcats 0.5.1
## v purrr   0.3.4</code></pre>
<pre><code>## Warning: package &#39;ggplot2&#39; was built under R version 4.1.3</code></pre>
<pre><code>## -- Conflicts ------------------------------------------ tidyverse_conflicts() --
## x dplyr::filter() masks stats::filter()
## x dplyr::lag()    masks stats::lag()</code></pre>
<div class="sourceCode" id="cb964"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb964-1"><a href="machine-learning.html#cb964-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(sf)</span></code></pre></div>
<pre><code>## Linking to GEOS 3.9.1, GDAL 3.2.1, PROJ 7.2.1; sf_use_s2() is TRUE</code></pre>
<div class="sourceCode" id="cb966"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb966-1"><a href="machine-learning.html#cb966-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(leaflet)</span>
<span id="cb966-2"><a href="machine-learning.html#cb966-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(RColorBrewer)</span></code></pre></div>
<pre><code>## Warning: package &#39;RColorBrewer&#39; was built under R version 4.1.3</code></pre>
<div class="sourceCode" id="cb968"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb968-1"><a href="machine-learning.html#cb968-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(htmlwidgets)</span>
<span id="cb968-2"><a href="machine-learning.html#cb968-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(webshot2)</span>
<span id="cb968-3"><a href="machine-learning.html#cb968-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb968-4"><a href="machine-learning.html#cb968-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb968-5"><a href="machine-learning.html#cb968-5" aria-hidden="true" tabindex="-1"></a>clean_data_w_geo <span class="ot">&lt;-</span> <span class="fu">st_read</span>(<span class="st">&quot;data-unit-13/midwest_county_data.shp&quot;</span>, <span class="at">quiet=</span><span class="cn">TRUE</span>) </span>
<span id="cb968-6"><a href="machine-learning.html#cb968-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb968-7"><a href="machine-learning.html#cb968-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb968-8"><a href="machine-learning.html#cb968-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb968-9"><a href="machine-learning.html#cb968-9" aria-hidden="true" tabindex="-1"></a>pal_ppt <span class="ot">=</span> <span class="fu">colorBin</span>(<span class="st">&quot;RdYlGn&quot;</span>, <span class="at">bins=</span><span class="dv">5</span>, clean_data_w_geo<span class="sc">$</span>ppt)</span>
<span id="cb968-10"><a href="machine-learning.html#cb968-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb968-11"><a href="machine-learning.html#cb968-11" aria-hidden="true" tabindex="-1"></a>m <span class="ot">&lt;-</span> clean_data_w_geo <span class="sc">%&gt;%</span></span>
<span id="cb968-12"><a href="machine-learning.html#cb968-12" aria-hidden="true" tabindex="-1"></a>  <span class="fu">leaflet</span>() <span class="sc">%&gt;%</span></span>
<span id="cb968-13"><a href="machine-learning.html#cb968-13" aria-hidden="true" tabindex="-1"></a>  <span class="fu">addTiles</span>() <span class="sc">%&gt;%</span></span>
<span id="cb968-14"><a href="machine-learning.html#cb968-14" aria-hidden="true" tabindex="-1"></a>  <span class="fu">addPolygons</span>(</span>
<span id="cb968-15"><a href="machine-learning.html#cb968-15" aria-hidden="true" tabindex="-1"></a>    <span class="at">fillColor =</span> <span class="sc">~</span><span class="fu">pal_ppt</span>(ppt),</span>
<span id="cb968-16"><a href="machine-learning.html#cb968-16" aria-hidden="true" tabindex="-1"></a>    <span class="at">fillOpacity =</span> <span class="fl">0.8</span>,</span>
<span id="cb968-17"><a href="machine-learning.html#cb968-17" aria-hidden="true" tabindex="-1"></a>    <span class="at">weight=</span><span class="dv">1</span>,</span>
<span id="cb968-18"><a href="machine-learning.html#cb968-18" aria-hidden="true" tabindex="-1"></a>    <span class="at">color =</span> <span class="st">&quot;black&quot;</span></span>
<span id="cb968-19"><a href="machine-learning.html#cb968-19" aria-hidden="true" tabindex="-1"></a>  ) <span class="sc">%&gt;%</span></span>
<span id="cb968-20"><a href="machine-learning.html#cb968-20" aria-hidden="true" tabindex="-1"></a>  <span class="fu">addLegend</span>(<span class="at">pal =</span> pal_ppt,</span>
<span id="cb968-21"><a href="machine-learning.html#cb968-21" aria-hidden="true" tabindex="-1"></a>            <span class="at">values =</span> <span class="sc">~</span>ppt</span>
<span id="cb968-22"><a href="machine-learning.html#cb968-22" aria-hidden="true" tabindex="-1"></a>  )</span>
<span id="cb968-23"><a href="machine-learning.html#cb968-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb968-24"><a href="machine-learning.html#cb968-24" aria-hidden="true" tabindex="-1"></a><span class="co">#above code won&#39;t knit -- created static image for markdown</span></span>
<span id="cb968-25"><a href="machine-learning.html#cb968-25" aria-hidden="true" tabindex="-1"></a><span class="fu">saveWidget</span>(m, <span class="st">&quot;temp.html&quot;</span>, <span class="at">selfcontained =</span> <span class="cn">FALSE</span>)</span>
<span id="cb968-26"><a href="machine-learning.html#cb968-26" aria-hidden="true" tabindex="-1"></a><span class="fu">webshot</span>(<span class="st">&quot;temp.html&quot;</span>, <span class="at">cliprect =</span> <span class="st">&quot;viewport&quot;</span>)</span></code></pre></div>
<p><img src="13-Machine-Learning_files/figure-html/unnamed-chunk-1-1.png" width="672" /></p>
<p>Next up, here is a plot of percentage sand. This is a fascinating map. We can see the path of the glaciers down through Iowa and the Des Moines lobe. Areas with low sand (red below) are largely below the southern reach of the glacier. These areas tend to be high in silt. Conversely, we can see the outwash areas (green below) of the Nebraska Sand Hills, central Minnesota, northern Wisconsin, and Western Michigan, where rapid glacial melting sorted parent material, leaving them higher in sand and rock fragments.</p>
<div class="sourceCode" id="cb969"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb969-1"><a href="machine-learning.html#cb969-1" aria-hidden="true" tabindex="-1"></a>pal_sand <span class="ot">=</span> <span class="fu">colorBin</span>(<span class="st">&quot;RdYlGn&quot;</span>, <span class="at">bins =</span> <span class="dv">5</span>, clean_data_w_geo<span class="sc">$</span>sand)</span>
<span id="cb969-2"><a href="machine-learning.html#cb969-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb969-3"><a href="machine-learning.html#cb969-3" aria-hidden="true" tabindex="-1"></a>m <span class="ot">&lt;-</span> clean_data_w_geo <span class="sc">%&gt;%</span></span>
<span id="cb969-4"><a href="machine-learning.html#cb969-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">leaflet</span>() <span class="sc">%&gt;%</span></span>
<span id="cb969-5"><a href="machine-learning.html#cb969-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">addTiles</span>() <span class="sc">%&gt;%</span></span>
<span id="cb969-6"><a href="machine-learning.html#cb969-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">addPolygons</span>(</span>
<span id="cb969-7"><a href="machine-learning.html#cb969-7" aria-hidden="true" tabindex="-1"></a>    <span class="at">fillColor =</span> <span class="sc">~</span><span class="fu">pal_sand</span>(sand),</span>
<span id="cb969-8"><a href="machine-learning.html#cb969-8" aria-hidden="true" tabindex="-1"></a>    <span class="at">fillOpacity =</span> <span class="fl">0.8</span>,</span>
<span id="cb969-9"><a href="machine-learning.html#cb969-9" aria-hidden="true" tabindex="-1"></a>    <span class="at">weight=</span><span class="dv">1</span>,</span>
<span id="cb969-10"><a href="machine-learning.html#cb969-10" aria-hidden="true" tabindex="-1"></a>    <span class="at">color =</span> <span class="st">&quot;black&quot;</span></span>
<span id="cb969-11"><a href="machine-learning.html#cb969-11" aria-hidden="true" tabindex="-1"></a>  ) <span class="sc">%&gt;%</span></span>
<span id="cb969-12"><a href="machine-learning.html#cb969-12" aria-hidden="true" tabindex="-1"></a>  <span class="fu">addLegend</span>(<span class="at">pal =</span> pal_sand,</span>
<span id="cb969-13"><a href="machine-learning.html#cb969-13" aria-hidden="true" tabindex="-1"></a>            <span class="at">values =</span> <span class="sc">~</span>sand</span>
<span id="cb969-14"><a href="machine-learning.html#cb969-14" aria-hidden="true" tabindex="-1"></a>  )</span>
<span id="cb969-15"><a href="machine-learning.html#cb969-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb969-16"><a href="machine-learning.html#cb969-16" aria-hidden="true" tabindex="-1"></a><span class="co">#above code won&#39;t knit -- created static image for markdown</span></span>
<span id="cb969-17"><a href="machine-learning.html#cb969-17" aria-hidden="true" tabindex="-1"></a><span class="fu">saveWidget</span>(m, <span class="st">&quot;temp.html&quot;</span>, <span class="at">selfcontained =</span> <span class="cn">FALSE</span>)</span>
<span id="cb969-18"><a href="machine-learning.html#cb969-18" aria-hidden="true" tabindex="-1"></a><span class="fu">webshot</span>(<span class="st">&quot;temp.html&quot;</span>, <span class="at">cliprect =</span> <span class="st">&quot;viewport&quot;</span>)</span></code></pre></div>
<p><img src="13-Machine-Learning_files/figure-html/unnamed-chunk-2-1.png" width="672" /></p>
<p>We can continue looking at different variables, and through that process might arrive at a general conclusions. Counties that are further south or east tend to receive greater precipitation and more growing degree days (GDD). Counties that are further north and east tend to have more sand and less silt or clay in their soils.</p>
<p>But where do these boundaries end? Say you have a product, like pyraclostrobin, that may reduce heat, and you want to test it across a range of environments that differ in temperature, precipitation and soil courseness. How would you sort your counties into three groups? This is where cluster analysis becomes very handy.</p>
</div>
<div id="scaling" class="section level3 hasAnchor" number="13.2.2">
<h3><span class="header-section-number">13.2.2</span> Scaling<a href="machine-learning.html#scaling" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>When we conduct a cluster analysis, we must remember to scale the data first. Clustering works by evaluating the distance between points. The distance between points <span class="math inline">\(p\)</span> and <span class="math inline">\(q\)</span> is calculated as:</p>
<p><span class="math display">\[dist(p,q) = \sqrt{(p_1 - q_1)^2 + (p_2 - q_2)^2 ... (p_n - q_n)^2} \]</span></p>
<p>Remember, this distance is Euclidean – it is the same concept by which we calculate the hypotenuse of a triangle, only in this case we are working with a many more dimensions. Euclidean distance is commonly described when we refer to “as the crow flies” – it is the shortest distance between two points.</p>
<p>Cluster analysis will work to minimise the overall distance. If one variable has a much wider range of values than another, it will be weighted more heavily by the clustering algorithm. For example, in our dataset above, growing degree days range from 1351 to 4441, a difference of almost 3100. Precipitation, on the other hand, only ranges from 253 to 694, a different of 341. Yet we would agree the that growing degree days and precipitation are equally important to growing corn!</p>
<p>The solution is to <em>scale</em> the data so that each variable has a comparable range of values. Below are the cumulative growing degree days, precipitation, and percent clay values for the first 10 counties in our dataset. You can see how they differ in the range of their values.</p>
<div class="sourceCode" id="cb970"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb970-1"><a href="machine-learning.html#cb970-1" aria-hidden="true" tabindex="-1"></a>cluster_vars_only <span class="ot">=</span> clean_data_w_geo <span class="sc">%&gt;%</span></span>
<span id="cb970-2"><a href="machine-learning.html#cb970-2" aria-hidden="true" tabindex="-1"></a>  dplyr<span class="sc">::</span><span class="fu">select</span>(whc, sand, silt, clay, om, ppt, cum_gdd) <span class="sc">%&gt;%</span></span>
<span id="cb970-3"><a href="machine-learning.html#cb970-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">na.omit</span>()</span>
<span id="cb970-4"><a href="machine-learning.html#cb970-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb970-5"><a href="machine-learning.html#cb970-5" aria-hidden="true" tabindex="-1"></a>cluster_vars_only <span class="sc">%&gt;%</span></span>
<span id="cb970-6"><a href="machine-learning.html#cb970-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">filter</span>(<span class="fu">row_number</span>() <span class="sc">&lt;=</span> <span class="dv">10</span>) <span class="sc">%&gt;%</span></span>
<span id="cb970-7"><a href="machine-learning.html#cb970-7" aria-hidden="true" tabindex="-1"></a>  dplyr<span class="sc">::</span><span class="fu">select</span>(cum_gdd, ppt, clay) <span class="sc">%&gt;%</span></span>
<span id="cb970-8"><a href="machine-learning.html#cb970-8" aria-hidden="true" tabindex="-1"></a>  kableExtra<span class="sc">::</span><span class="fu">kbl</span>()</span></code></pre></div>
<pre><code>## Registered S3 method overwritten by &#39;webshot&#39;:
##   method        from    
##   print.webshot webshot2</code></pre>
<table>
<thead>
<tr>
<th style="text-align:right;">
cum_gdd
</th>
<th style="text-align:right;">
ppt
</th>
<th style="text-align:right;">
clay
</th>
<th style="text-align:left;">
geometry
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:right;">
3140.368
</td>
<td style="text-align:right;">
598.4478
</td>
<td style="text-align:right;">
33.73933
</td>
<td style="text-align:left;">
MULTIPOLYGON (((-94.70063 4…
</td>
</tr>
<tr>
<td style="text-align:right;">
3145.911
</td>
<td style="text-align:right;">
609.1156
</td>
<td style="text-align:right;">
34.32862
</td>
<td style="text-align:left;">
MULTIPOLYGON (((-94.92759 4…
</td>
</tr>
<tr>
<td style="text-align:right;">
2633.780
</td>
<td style="text-align:right;">
599.4642
</td>
<td style="text-align:right;">
28.56537
</td>
<td style="text-align:left;">
MULTIPOLYGON (((-91.61083 4…
</td>
</tr>
<tr>
<td style="text-align:right;">
3241.873
</td>
<td style="text-align:right;">
659.3385
</td>
<td style="text-align:right;">
37.16523
</td>
<td style="text-align:left;">
MULTIPOLYGON (((-93.09759 4…
</td>
</tr>
<tr>
<td style="text-align:right;">
2989.952
</td>
<td style="text-align:right;">
598.6462
</td>
<td style="text-align:right;">
30.46776
</td>
<td style="text-align:left;">
MULTIPOLYGON (((-95.09286 4…
</td>
</tr>
<tr>
<td style="text-align:right;">
2869.339
</td>
<td style="text-align:right;">
606.5850
</td>
<td style="text-align:right;">
26.67489
</td>
<td style="text-align:left;">
MULTIPOLYGON (((-92.29879 4…
</td>
</tr>
<tr>
<td style="text-align:right;">
2796.425
</td>
<td style="text-align:right;">
612.5765
</td>
<td style="text-align:right;">
21.99091
</td>
<td style="text-align:left;">
MULTIPOLYGON (((-92.55449 4…
</td>
</tr>
<tr>
<td style="text-align:right;">
2971.723
</td>
<td style="text-align:right;">
611.0333
</td>
<td style="text-align:right;">
21.81959
</td>
<td style="text-align:left;">
MULTIPOLYGON (((-94.1647 42…
</td>
</tr>
<tr>
<td style="text-align:right;">
2727.186
</td>
<td style="text-align:right;">
618.2398
</td>
<td style="text-align:right;">
21.67287
</td>
<td style="text-align:left;">
MULTIPOLYGON (((-92.55421 4…
</td>
</tr>
<tr>
<td style="text-align:right;">
2683.756
</td>
<td style="text-align:right;">
620.8813
</td>
<td style="text-align:right;">
21.66446
</td>
<td style="text-align:left;">
MULTIPOLYGON (((-92.08166 4…
</td>
</tr>
</tbody>
</table>
<p>Here are those same 10 counties, with their values for cum_gdd, ppt, and clay scaled. One of the easiest ways to scale the data are to convert their original values to Z-scores, based on their normal distributions. We do this exactly the way we calculated Z-scores in Unit 2.</p>
<p>We can see, for each variable, values fall mainly between -1 and 1.</p>
<div class="sourceCode" id="cb972"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb972-1"><a href="machine-learning.html#cb972-1" aria-hidden="true" tabindex="-1"></a>cluster_ready_data <span class="ot">=</span> cluster_vars_only <span class="sc">%&gt;%</span></span>
<span id="cb972-2"><a href="machine-learning.html#cb972-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">st_drop_geometry</span>() <span class="sc">%&gt;%</span></span>
<span id="cb972-3"><a href="machine-learning.html#cb972-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">as.data.frame</span>() <span class="sc">%&gt;%</span></span>
<span id="cb972-4"><a href="machine-learning.html#cb972-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">scale</span>()</span>
<span id="cb972-5"><a href="machine-learning.html#cb972-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb972-6"><a href="machine-learning.html#cb972-6" aria-hidden="true" tabindex="-1"></a>cluster_ready_data <span class="sc">%&gt;%</span></span>
<span id="cb972-7"><a href="machine-learning.html#cb972-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">as.data.frame</span>() <span class="sc">%&gt;%</span></span>
<span id="cb972-8"><a href="machine-learning.html#cb972-8" aria-hidden="true" tabindex="-1"></a>  <span class="fu">filter</span>(<span class="fu">row_number</span>() <span class="sc">&lt;=</span> <span class="dv">10</span>) <span class="sc">%&gt;%</span></span>
<span id="cb972-9"><a href="machine-learning.html#cb972-9" aria-hidden="true" tabindex="-1"></a>  dplyr<span class="sc">::</span><span class="fu">select</span>(cum_gdd, ppt, clay)</span></code></pre></div>
<pre><code>##        cum_gdd       ppt       clay
## 1   0.55154310 0.8572820  1.1905194
## 2   0.56106037 0.9708866  1.2651893
## 3  -0.31828780 0.8681063  0.5349214
## 4   0.72583066 1.5057241  1.6246202
## 5   0.29327347 0.8593956  0.7759755
## 6   0.08617543 0.9439380  0.2953761
## 7  -0.03902028 1.0077425 -0.2981361
## 8   0.26197313 0.9913093 -0.3198444
## 9  -0.15790581 1.0680532 -0.3384352
## 10 -0.23247627 1.0961831 -0.3395009</code></pre>
</div>
<div id="clustering-animation" class="section level3 hasAnchor" number="13.2.3">
<h3><span class="header-section-number">13.2.3</span> Clustering Animation<a href="machine-learning.html#clustering-animation" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div class="sourceCode" id="cb974"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb974-1"><a href="machine-learning.html#cb974-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(animation)</span>
<span id="cb974-2"><a href="machine-learning.html#cb974-2" aria-hidden="true" tabindex="-1"></a>km_animation_data <span class="ot">=</span> cluster_ready_data <span class="sc">%&gt;%</span></span>
<span id="cb974-3"><a href="machine-learning.html#cb974-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">as.data.frame</span>() <span class="sc">%&gt;%</span></span>
<span id="cb974-4"><a href="machine-learning.html#cb974-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">sample_n</span>(<span class="dv">45</span>) <span class="sc">%&gt;%</span></span>
<span id="cb974-5"><a href="machine-learning.html#cb974-5" aria-hidden="true" tabindex="-1"></a>  dplyr<span class="sc">::</span><span class="fu">select</span>(cum_gdd, silt) <span class="sc">%&gt;%</span></span>
<span id="cb974-6"><a href="machine-learning.html#cb974-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">as.matrix</span>()</span></code></pre></div>
<p>Using two variable, cum_gdd and silt and the animation below, lets walk through how clustering works.</p>
<p>In the plot above, cum_gdd is on the X-axis and silt is on the Y-axis. We want to cluster (or divide) our points inth three groups. Colors have been assigned randomly to the points (smaller circles). They are scattered instead of being grouped by proximity to each other. After clustering, the points will be colored according to group.</p>
<p>The first step is to randomly place three <em>centroids</em> in our plot. These are indicated by the large circles.</p>
<p><img src="data-unit-13/cluster_frames/frame_00_delay-1s.jpg" /></p>
<ol start="2" style="list-style-type: decimal">
<li>Next, for each point, we classify each point by its nearest centroid. In the plot below, this is represented by each point taking the same color as its nearest centroid.</li>
</ol>
<p><img src="data-unit-13/cluster_frames/frame_01_delay-1s.jpg" /></p>
<ol start="3" style="list-style-type: decimal">
<li>But now our centroid is not in the center of the group. So we will move it to the center of the group, as seen in the plot below.</li>
</ol>
<p><img src="data-unit-13/cluster_frames/frame_02_delay-1s.jpg" /></p>
<p>As soon as we move the centroids to the middle of its group, however, a problem occurs. Remember, each point is identifed by the nearest centroid. So the groupings of the points again changes. In particular, the number of points in the red grouping increases.</p>
<p><img src="data-unit-13/cluster_frames/frame_03_delay-1s.jpg" /></p>
<p>We move our centroids again. You will notice both the red and blue centroids have moved down and to the left.</p>
<p><img src="data-unit-13/cluster_frames/frame_04_delay-1s.jpg" /></p>
<p>We reclassify the points. The number of red points has again increased.</p>
<p><img src="data-unit-13/cluster_frames/frame_05_delay-1s.jpg" /></p>
<p>This process repeats over and over again until, eventually, there is no more movement in the centroids. At this point, the cluster analysis is said to have converged on its final groupings. The positions of the centroids are typically used to define the new groupings.</p>
<div class="sourceCode" id="cb975"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb975-1"><a href="machine-learning.html#cb975-1" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">112020</span>)</span>
<span id="cb975-2"><a href="machine-learning.html#cb975-2" aria-hidden="true" tabindex="-1"></a>kmean_silt_gdd <span class="ot">=</span> <span class="fu">kmeans</span>(km_animation_data, <span class="dv">3</span>)</span>
<span id="cb975-3"><a href="machine-learning.html#cb975-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb975-4"><a href="machine-learning.html#cb975-4" aria-hidden="true" tabindex="-1"></a>kmean_silt_gdd<span class="sc">$</span>centers <span class="sc">%&gt;%</span></span>
<span id="cb975-5"><a href="machine-learning.html#cb975-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">as.data.frame</span>() <span class="sc">%&gt;%</span></span>
<span id="cb975-6"><a href="machine-learning.html#cb975-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">rownames_to_column</span>(<span class="st">&quot;cluster&quot;</span>) <span class="sc">%&gt;%</span></span>
<span id="cb975-7"><a href="machine-learning.html#cb975-7" aria-hidden="true" tabindex="-1"></a>  kableExtra<span class="sc">::</span><span class="fu">kbl</span>()</span></code></pre></div>
<table>
<thead>
<tr>
<th style="text-align:left;">
cluster
</th>
<th style="text-align:right;">
cum_gdd
</th>
<th style="text-align:right;">
silt
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
1
</td>
<td style="text-align:right;">
-0.0809096
</td>
<td style="text-align:right;">
0.6244699
</td>
</tr>
<tr>
<td style="text-align:left;">
2
</td>
<td style="text-align:right;">
1.2183990
</td>
<td style="text-align:right;">
1.1785995
</td>
</tr>
<tr>
<td style="text-align:left;">
3
</td>
<td style="text-align:right;">
-0.6229289
</td>
<td style="text-align:right;">
-0.9538640
</td>
</tr>
</tbody>
</table>
<p>The table above contains the final center estimates from the cluster algorithm. Cluster 1 is warmer and has siltier soil. Cluster 2 has medium numbers of growing degree days and silt. Cluster 3 is cooler and has soils lower in clay.</p>
</div>
<div id="county-cluster-analysis" class="section level3 hasAnchor" number="13.2.4">
<h3><span class="header-section-number">13.2.4</span> County Cluster Analysis<a href="machine-learning.html#county-cluster-analysis" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Now, let’s run the cluster analysis with all of our variables. The cluster enters within each variable are shown below.</p>
<div class="sourceCode" id="cb976"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb976-1"><a href="machine-learning.html#cb976-1" aria-hidden="true" tabindex="-1"></a><span class="co"># cluster identification</span></span>
<span id="cb976-2"><a href="machine-learning.html#cb976-2" aria-hidden="true" tabindex="-1"></a>clusters <span class="ot">=</span> <span class="fu">kmeans</span>(cluster_ready_data, <span class="dv">3</span>)</span>
<span id="cb976-3"><a href="machine-learning.html#cb976-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb976-4"><a href="machine-learning.html#cb976-4" aria-hidden="true" tabindex="-1"></a>clusters<span class="sc">$</span>centers <span class="sc">%&gt;%</span></span>
<span id="cb976-5"><a href="machine-learning.html#cb976-5" aria-hidden="true" tabindex="-1"></a>  kableExtra<span class="sc">::</span><span class="fu">kbl</span>() </span></code></pre></div>
<table>
<thead>
<tr>
<th style="text-align:right;">
whc
</th>
<th style="text-align:right;">
sand
</th>
<th style="text-align:right;">
silt
</th>
<th style="text-align:right;">
clay
</th>
<th style="text-align:right;">
om
</th>
<th style="text-align:right;">
ppt
</th>
<th style="text-align:right;">
cum_gdd
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:right;">
-0.0152348
</td>
<td style="text-align:right;">
0.1147455
</td>
<td style="text-align:right;">
-0.3219194
</td>
<td style="text-align:right;">
0.2808926
</td>
<td style="text-align:right;">
0.1693996
</td>
<td style="text-align:right;">
-0.7947552
</td>
<td style="text-align:right;">
-0.6478638
</td>
</tr>
<tr>
<td style="text-align:right;">
0.4188589
</td>
<td style="text-align:right;">
-0.7234396
</td>
<td style="text-align:right;">
0.7811347
</td>
<td style="text-align:right;">
0.4051389
</td>
<td style="text-align:right;">
-0.4065930
</td>
<td style="text-align:right;">
0.7064892
</td>
<td style="text-align:right;">
0.7585691
</td>
</tr>
<tr>
<td style="text-align:right;">
-1.0435272
</td>
<td style="text-align:right;">
1.6497023
</td>
<td style="text-align:right;">
-1.4394576
</td>
<td style="text-align:right;">
-1.5196343
</td>
<td style="text-align:right;">
0.7460924
</td>
<td style="text-align:right;">
-0.4326339
</td>
<td style="text-align:right;">
-0.8192046
</td>
</tr>
</tbody>
</table>
<div class="sourceCode" id="cb977"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb977-1"><a href="machine-learning.html#cb977-1" aria-hidden="true" tabindex="-1"></a><span class="co"># classify centers and rejoin with original data</span></span>
<span id="cb977-2"><a href="machine-learning.html#cb977-2" aria-hidden="true" tabindex="-1"></a>centers <span class="ot">=</span> <span class="fu">as.data.frame</span>(clusters<span class="sc">$</span>centers) <span class="sc">%&gt;%</span></span>
<span id="cb977-3"><a href="machine-learning.html#cb977-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">rownames_to_column</span>(<span class="st">&quot;cluster&quot;</span>) <span class="sc">%&gt;%</span></span>
<span id="cb977-4"><a href="machine-learning.html#cb977-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">gather</span>(attribute, value, <span class="sc">-</span>cluster) <span class="sc">%&gt;%</span></span>
<span id="cb977-5"><a href="machine-learning.html#cb977-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">named_value =</span> <span class="fu">case_when</span>(value <span class="sc">&lt;</span> <span class="sc">-</span><span class="fl">0.45</span> <span class="sc">~</span> <span class="st">&quot;low&quot;</span>,</span>
<span id="cb977-6"><a href="machine-learning.html#cb977-6" aria-hidden="true" tabindex="-1"></a>                                 value <span class="sc">&gt;</span> <span class="fl">0.45</span> <span class="sc">~</span> <span class="st">&quot;high&quot;</span>,</span>
<span id="cb977-7"><a href="machine-learning.html#cb977-7" aria-hidden="true" tabindex="-1"></a>                                 value <span class="sc">&gt;=</span> <span class="sc">-</span><span class="fl">0.45</span> <span class="sc">&amp;</span> value <span class="sc">&lt;=</span><span class="fl">0.45</span> <span class="sc">~</span> <span class="st">&quot;ave&quot;</span>)) <span class="sc">%&gt;%</span></span>
<span id="cb977-8"><a href="machine-learning.html#cb977-8" aria-hidden="true" tabindex="-1"></a>  dplyr<span class="sc">::</span><span class="fu">select</span>(<span class="sc">-</span>value) <span class="sc">%&gt;%</span></span>
<span id="cb977-9"><a href="machine-learning.html#cb977-9" aria-hidden="true" tabindex="-1"></a>  <span class="fu">spread</span>(attribute, named_value) <span class="sc">%&gt;%</span></span>
<span id="cb977-10"><a href="machine-learning.html#cb977-10" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">cluster =</span> <span class="fu">as.numeric</span>(cluster)) <span class="sc">%&gt;%</span></span>
<span id="cb977-11"><a href="machine-learning.html#cb977-11" aria-hidden="true" tabindex="-1"></a>  <span class="fu">rename</span>(<span class="at">om_class =</span> om,</span>
<span id="cb977-12"><a href="machine-learning.html#cb977-12" aria-hidden="true" tabindex="-1"></a>         <span class="at">ppt_class =</span> ppt,</span>
<span id="cb977-13"><a href="machine-learning.html#cb977-13" aria-hidden="true" tabindex="-1"></a>         <span class="at">clay_class =</span> clay,</span>
<span id="cb977-14"><a href="machine-learning.html#cb977-14" aria-hidden="true" tabindex="-1"></a>         <span class="at">silt_class =</span> silt,</span>
<span id="cb977-15"><a href="machine-learning.html#cb977-15" aria-hidden="true" tabindex="-1"></a>         <span class="at">sand_class =</span> sand,</span>
<span id="cb977-16"><a href="machine-learning.html#cb977-16" aria-hidden="true" tabindex="-1"></a>         <span class="at">whc_class =</span> whc,</span>
<span id="cb977-17"><a href="machine-learning.html#cb977-17" aria-hidden="true" tabindex="-1"></a>         <span class="at">gdd_class =</span> cum_gdd)</span>
<span id="cb977-18"><a href="machine-learning.html#cb977-18" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb977-19"><a href="machine-learning.html#cb977-19" aria-hidden="true" tabindex="-1"></a>cluster_df <span class="ot">&lt;-</span> clusters<span class="sc">$</span>cluster <span class="sc">%&gt;%</span></span>
<span id="cb977-20"><a href="machine-learning.html#cb977-20" aria-hidden="true" tabindex="-1"></a>  <span class="fu">enframe</span>(<span class="at">name=</span><span class="cn">NULL</span>, <span class="at">value=</span><span class="st">&quot;cluster&quot;</span>)</span>
<span id="cb977-21"><a href="machine-learning.html#cb977-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb977-22"><a href="machine-learning.html#cb977-22" aria-hidden="true" tabindex="-1"></a>classified_data <span class="ot">&lt;-</span>  cluster_ready_data <span class="sc">%&gt;%</span></span>
<span id="cb977-23"><a href="machine-learning.html#cb977-23" aria-hidden="true" tabindex="-1"></a>  <span class="fu">cbind</span>(cluster_df) <span class="sc">%&gt;%</span></span>
<span id="cb977-24"><a href="machine-learning.html#cb977-24" aria-hidden="true" tabindex="-1"></a>  <span class="fu">left_join</span>(centers, <span class="at">by=</span><span class="st">&quot;cluster&quot;</span>) </span>
<span id="cb977-25"><a href="machine-learning.html#cb977-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb977-26"><a href="machine-learning.html#cb977-26" aria-hidden="true" tabindex="-1"></a>centers <span class="sc">%&gt;%</span></span>
<span id="cb977-27"><a href="machine-learning.html#cb977-27" aria-hidden="true" tabindex="-1"></a>  kableExtra<span class="sc">::</span><span class="fu">kbl</span>()</span></code></pre></div>
<table>
<thead>
<tr>
<th style="text-align:right;">
cluster
</th>
<th style="text-align:left;">
clay_class
</th>
<th style="text-align:left;">
gdd_class
</th>
<th style="text-align:left;">
om_class
</th>
<th style="text-align:left;">
ppt_class
</th>
<th style="text-align:left;">
sand_class
</th>
<th style="text-align:left;">
silt_class
</th>
<th style="text-align:left;">
whc_class
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:right;">
1
</td>
<td style="text-align:left;">
ave
</td>
<td style="text-align:left;">
low
</td>
<td style="text-align:left;">
ave
</td>
<td style="text-align:left;">
low
</td>
<td style="text-align:left;">
ave
</td>
<td style="text-align:left;">
ave
</td>
<td style="text-align:left;">
ave
</td>
</tr>
<tr>
<td style="text-align:right;">
2
</td>
<td style="text-align:left;">
ave
</td>
<td style="text-align:left;">
high
</td>
<td style="text-align:left;">
ave
</td>
<td style="text-align:left;">
high
</td>
<td style="text-align:left;">
low
</td>
<td style="text-align:left;">
high
</td>
<td style="text-align:left;">
ave
</td>
</tr>
<tr>
<td style="text-align:right;">
3
</td>
<td style="text-align:left;">
low
</td>
<td style="text-align:left;">
low
</td>
<td style="text-align:left;">
high
</td>
<td style="text-align:left;">
ave
</td>
<td style="text-align:left;">
high
</td>
<td style="text-align:left;">
low
</td>
<td style="text-align:left;">
low
</td>
</tr>
</tbody>
</table>
<p>We can now plot our clusters on a map. Cluster 1, which has a greater growing degree days, precipitation and silt, accounts for much of the lower third of our map. Cluster 2, which has lower growing degree days and precipitation, accounts for most of northwestern quarter of counties. Cluster 3, which is most remarkable for its higher sand content, accounts for parts of Nebraska, central Wisconsin and counties to the south and east of Lake Michigan.</p>
<div class="sourceCode" id="cb978"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb978-1"><a href="machine-learning.html#cb978-1" aria-hidden="true" tabindex="-1"></a>classified_data_w_geo <span class="ot">=</span> clean_data_w_geo <span class="sc">%&gt;%</span></span>
<span id="cb978-2"><a href="machine-learning.html#cb978-2" aria-hidden="true" tabindex="-1"></a>  dplyr<span class="sc">::</span><span class="fu">select</span>(stco, geometry) <span class="sc">%&gt;%</span></span>
<span id="cb978-3"><a href="machine-learning.html#cb978-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">cbind</span>(classified_data) <span class="sc">%&gt;%</span></span>
<span id="cb978-4"><a href="machine-learning.html#cb978-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">st_as_sf</span>()</span>
<span id="cb978-5"><a href="machine-learning.html#cb978-5" aria-hidden="true" tabindex="-1"></a>classified_data_w_geo <span class="ot">=</span> <span class="fu">st_transform</span>(classified_data_w_geo, <span class="dv">4326</span>)</span>
<span id="cb978-6"><a href="machine-learning.html#cb978-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb978-7"><a href="machine-learning.html#cb978-7" aria-hidden="true" tabindex="-1"></a>pal_ppt <span class="ot">=</span> <span class="fu">colorFactor</span>(<span class="st">&quot;RdYlGn&quot;</span>, classified_data_w_geo<span class="sc">$</span>cluster)</span>
<span id="cb978-8"><a href="machine-learning.html#cb978-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb978-9"><a href="machine-learning.html#cb978-9" aria-hidden="true" tabindex="-1"></a>m <span class="ot">&lt;-</span> classified_data_w_geo <span class="sc">%&gt;%</span></span>
<span id="cb978-10"><a href="machine-learning.html#cb978-10" aria-hidden="true" tabindex="-1"></a>  <span class="fu">leaflet</span>() <span class="sc">%&gt;%</span></span>
<span id="cb978-11"><a href="machine-learning.html#cb978-11" aria-hidden="true" tabindex="-1"></a>  <span class="fu">addTiles</span>() <span class="sc">%&gt;%</span></span>
<span id="cb978-12"><a href="machine-learning.html#cb978-12" aria-hidden="true" tabindex="-1"></a>  <span class="fu">addPolygons</span>(</span>
<span id="cb978-13"><a href="machine-learning.html#cb978-13" aria-hidden="true" tabindex="-1"></a>    <span class="at">fillColor =</span> <span class="sc">~</span><span class="fu">pal_ppt</span>(cluster),</span>
<span id="cb978-14"><a href="machine-learning.html#cb978-14" aria-hidden="true" tabindex="-1"></a>    <span class="at">fillOpacity =</span> <span class="fl">0.8</span>,</span>
<span id="cb978-15"><a href="machine-learning.html#cb978-15" aria-hidden="true" tabindex="-1"></a>    <span class="at">weight=</span><span class="dv">1</span>,</span>
<span id="cb978-16"><a href="machine-learning.html#cb978-16" aria-hidden="true" tabindex="-1"></a>    <span class="at">color =</span> <span class="st">&quot;black&quot;</span>,</span>
<span id="cb978-17"><a href="machine-learning.html#cb978-17" aria-hidden="true" tabindex="-1"></a>    <span class="at">popup =</span> <span class="fu">paste0</span>(<span class="st">&quot;om: &quot;</span>, classified_data_w_geo<span class="sc">$</span>om_class, <span class="st">&quot;&lt;br&gt;&quot;</span>,</span>
<span id="cb978-18"><a href="machine-learning.html#cb978-18" aria-hidden="true" tabindex="-1"></a>                   <span class="st">&quot;ppt: &quot;</span>, classified_data_w_geo<span class="sc">$</span>ppt_class, <span class="st">&quot;&lt;br&gt;&quot;</span>,</span>
<span id="cb978-19"><a href="machine-learning.html#cb978-19" aria-hidden="true" tabindex="-1"></a>                   <span class="st">&quot;clay: &quot;</span>, classified_data_w_geo<span class="sc">$</span>clay_class, <span class="st">&quot;&lt;br&gt;&quot;</span>, </span>
<span id="cb978-20"><a href="machine-learning.html#cb978-20" aria-hidden="true" tabindex="-1"></a>                   <span class="st">&quot;silt: &quot;</span>, classified_data_w_geo<span class="sc">$</span>silt_class, <span class="st">&quot;&lt;br&gt;&quot;</span>,</span>
<span id="cb978-21"><a href="machine-learning.html#cb978-21" aria-hidden="true" tabindex="-1"></a>                   <span class="st">&quot;sand: &quot;</span>, classified_data_w_geo<span class="sc">$</span>sand_class, <span class="st">&quot;&lt;br&gt;&quot;</span>,</span>
<span id="cb978-22"><a href="machine-learning.html#cb978-22" aria-hidden="true" tabindex="-1"></a>                   <span class="st">&quot;whc: &quot;</span>, classified_data_w_geo<span class="sc">$</span>whc_class, <span class="st">&quot;&lt;br&gt;&quot;</span>,</span>
<span id="cb978-23"><a href="machine-learning.html#cb978-23" aria-hidden="true" tabindex="-1"></a>                   <span class="st">&quot;gdd: &quot;</span>, classified_data_w_geo<span class="sc">$</span>gdd_class)</span>
<span id="cb978-24"><a href="machine-learning.html#cb978-24" aria-hidden="true" tabindex="-1"></a>  ) <span class="sc">%&gt;%</span></span>
<span id="cb978-25"><a href="machine-learning.html#cb978-25" aria-hidden="true" tabindex="-1"></a>  <span class="fu">addLegend</span>(<span class="at">pal =</span> pal_ppt,</span>
<span id="cb978-26"><a href="machine-learning.html#cb978-26" aria-hidden="true" tabindex="-1"></a>            <span class="at">values =</span> <span class="sc">~</span>cluster</span>
<span id="cb978-27"><a href="machine-learning.html#cb978-27" aria-hidden="true" tabindex="-1"></a>  )</span>
<span id="cb978-28"><a href="machine-learning.html#cb978-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb978-29"><a href="machine-learning.html#cb978-29" aria-hidden="true" tabindex="-1"></a><span class="co">#above code won&#39;t knit -- created static image for markdown</span></span>
<span id="cb978-30"><a href="machine-learning.html#cb978-30" aria-hidden="true" tabindex="-1"></a><span class="fu">saveWidget</span>(m, <span class="st">&quot;temp.html&quot;</span>, <span class="at">selfcontained =</span> <span class="cn">FALSE</span>)</span>
<span id="cb978-31"><a href="machine-learning.html#cb978-31" aria-hidden="true" tabindex="-1"></a><span class="fu">webshot</span>(<span class="st">&quot;temp.html&quot;</span>, <span class="at">cliprect =</span> <span class="st">&quot;viewport&quot;</span>)</span></code></pre></div>
<p><img src="13-Machine-Learning_files/figure-html/unnamed-chunk-10-1.png" width="672" /></p>
</div>
</div>
<div id="k-nearest-neighbors" class="section level2 hasAnchor" number="13.3">
<h2><span class="header-section-number">13.3</span> k-Nearest-Neighbors<a href="machine-learning.html#k-nearest-neighbors" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>In <em>k-Nearest-Neighbors</em> (kNN) analyses, we again use the Euclidian distance between points. This time, however, we are not trying to cluster points, but to guess the value of a variable one observation, based on how similar it is to other observations.</p>
<div id="case-study-guessing-county-yields-based-on-environmental-similarity" class="section level3 hasAnchor" number="13.3.1">
<h3><span class="header-section-number">13.3.1</span> Case Study: Guessing County Yields based on Environmental Similarity<a href="machine-learning.html#case-study-guessing-county-yields-based-on-environmental-similarity" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>For this example, we will continue to work with our county environmental dataset. This dataset includes yields for corn, soybean, and, where applicable, wheat and cotton. What if, however, our corn yield data were incomplete: that is, not every county had a recorded yield for corn. How might we go about guessing/estimating/predicting what that corn yield might be?</p>
<p>Well, if you are a farmer and you are interested in a new product – what might you do? You might ask around (at least the farmers who aren’t bidding against you for rented land) and see whether any of them have used the product and what response they observed.</p>
<p>In considering their results, however, you might incorporate additional information, namely: how similar are their farming practices to yours? Are they using the same amount of nitrogen fertilizer? Do they apply it using the same practices (preplant vs split) as you? Are they planting the same hybrid? What is their crop rotation? What is their tillage system?</p>
<p>In other words, as you ask around about others experiences with that product, you are also, at least intuitively, going to weigh their experiences based on how similar their practices are to yours. (Come to think of it, this would be great dataset to mock up for a future semester!)</p>
<p>In other words, you would predict your success based on your nearest neighbors, either by physical distance to your farm, or by the similarity (closeness) of your production practices. If that makes sense, you now understand the concept of nearest neighbors.</p>
<p>For our county data scenario, we will simulate a situation where 20% of our counties are, at random, missing a measure for corn yield.</p>
<div class="sourceCode" id="cb979"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb979-1"><a href="machine-learning.html#cb979-1" aria-hidden="true" tabindex="-1"></a>yield_and_env <span class="ot">=</span> clean_data_w_geo</span></code></pre></div>
<div class="sourceCode" id="cb980"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb980-1"><a href="machine-learning.html#cb980-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(caret)</span></code></pre></div>
<pre><code>## Loading required package: lattice</code></pre>
<pre><code>## 
## Attaching package: &#39;caret&#39;</code></pre>
<pre><code>## The following object is masked from &#39;package:purrr&#39;:
## 
##     lift</code></pre>
<div class="sourceCode" id="cb984"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb984-1"><a href="machine-learning.html#cb984-1" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">112120</span>)</span>
<span id="cb984-2"><a href="machine-learning.html#cb984-2" aria-hidden="true" tabindex="-1"></a>trainingRows <span class="ot">=</span> <span class="fu">createDataPartition</span>(yield_and_env<span class="sc">$</span>corn, <span class="at">p=</span><span class="fl">0.80</span>, <span class="at">list =</span> <span class="cn">FALSE</span>)</span>
<span id="cb984-3"><a href="machine-learning.html#cb984-3" aria-hidden="true" tabindex="-1"></a>trainPartition <span class="ot">=</span> yield_and_env[trainingRows,]</span>
<span id="cb984-4"><a href="machine-learning.html#cb984-4" aria-hidden="true" tabindex="-1"></a>trainData <span class="ot">=</span> trainPartition <span class="sc">%&gt;%</span></span>
<span id="cb984-5"><a href="machine-learning.html#cb984-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">st_drop_geometry</span>() <span class="sc">%&gt;%</span></span>
<span id="cb984-6"><a href="machine-learning.html#cb984-6" aria-hidden="true" tabindex="-1"></a>  dplyr<span class="sc">::</span><span class="fu">select</span>(<span class="sc">-</span>stco)</span>
<span id="cb984-7"><a href="machine-learning.html#cb984-7" aria-hidden="true" tabindex="-1"></a>testData <span class="ot">=</span> yield_and_env[<span class="sc">-</span>trainingRows,] </span></code></pre></div>
<div class="sourceCode" id="cb985"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb985-1"><a href="machine-learning.html#cb985-1" aria-hidden="true" tabindex="-1"></a>trainPartition <span class="ot">=</span> <span class="fu">st_transform</span>(trainPartition, <span class="dv">4326</span>)</span>
<span id="cb985-2"><a href="machine-learning.html#cb985-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb985-3"><a href="machine-learning.html#cb985-3" aria-hidden="true" tabindex="-1"></a>pal_corn <span class="ot">=</span> <span class="fu">colorBin</span>(<span class="st">&quot;RdYlGn&quot;</span>, <span class="at">bins=</span><span class="dv">5</span>, trainPartition<span class="sc">$</span>corn)</span>
<span id="cb985-4"><a href="machine-learning.html#cb985-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb985-5"><a href="machine-learning.html#cb985-5" aria-hidden="true" tabindex="-1"></a>m <span class="ot">&lt;-</span> trainPartition <span class="sc">%&gt;%</span></span>
<span id="cb985-6"><a href="machine-learning.html#cb985-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">leaflet</span>() <span class="sc">%&gt;%</span></span>
<span id="cb985-7"><a href="machine-learning.html#cb985-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">addTiles</span>() <span class="sc">%&gt;%</span></span>
<span id="cb985-8"><a href="machine-learning.html#cb985-8" aria-hidden="true" tabindex="-1"></a>  <span class="fu">addPolygons</span>(</span>
<span id="cb985-9"><a href="machine-learning.html#cb985-9" aria-hidden="true" tabindex="-1"></a>    <span class="at">fillColor =</span> <span class="sc">~</span><span class="fu">pal_corn</span>(corn),</span>
<span id="cb985-10"><a href="machine-learning.html#cb985-10" aria-hidden="true" tabindex="-1"></a>    <span class="at">fillOpacity =</span> <span class="fl">0.8</span>,</span>
<span id="cb985-11"><a href="machine-learning.html#cb985-11" aria-hidden="true" tabindex="-1"></a>    <span class="at">weight=</span><span class="dv">1</span>,</span>
<span id="cb985-12"><a href="machine-learning.html#cb985-12" aria-hidden="true" tabindex="-1"></a>    <span class="at">color =</span> <span class="st">&quot;black&quot;</span>,</span>
<span id="cb985-13"><a href="machine-learning.html#cb985-13" aria-hidden="true" tabindex="-1"></a>    <span class="at">popup =</span> <span class="fu">paste</span>(<span class="st">&quot;yield:&quot;</span>, <span class="fu">as.character</span>(<span class="fu">round</span>(trainPartition<span class="sc">$</span>corn,<span class="dv">1</span>)))</span>
<span id="cb985-14"><a href="machine-learning.html#cb985-14" aria-hidden="true" tabindex="-1"></a>  ) <span class="sc">%&gt;%</span></span>
<span id="cb985-15"><a href="machine-learning.html#cb985-15" aria-hidden="true" tabindex="-1"></a>  <span class="fu">addLegend</span>(<span class="at">pal =</span> pal_corn,</span>
<span id="cb985-16"><a href="machine-learning.html#cb985-16" aria-hidden="true" tabindex="-1"></a>            <span class="at">values =</span> <span class="sc">~</span>ppt</span>
<span id="cb985-17"><a href="machine-learning.html#cb985-17" aria-hidden="true" tabindex="-1"></a>  )</span>
<span id="cb985-18"><a href="machine-learning.html#cb985-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb985-19"><a href="machine-learning.html#cb985-19" aria-hidden="true" tabindex="-1"></a><span class="co">#above code won&#39;t knit -- created static image for markdown</span></span>
<span id="cb985-20"><a href="machine-learning.html#cb985-20" aria-hidden="true" tabindex="-1"></a><span class="fu">saveWidget</span>(m, <span class="st">&quot;temp.html&quot;</span>, <span class="at">selfcontained =</span> <span class="cn">FALSE</span>)</span>
<span id="cb985-21"><a href="machine-learning.html#cb985-21" aria-hidden="true" tabindex="-1"></a><span class="fu">webshot</span>(<span class="st">&quot;temp.html&quot;</span>, <span class="at">cliprect =</span> <span class="st">&quot;viewport&quot;</span>)</span></code></pre></div>
<p><img src="13-Machine-Learning_files/figure-html/unnamed-chunk-13-1.png" width="672" /></p>
</div>
<div id="scaling-1" class="section level3 hasAnchor" number="13.3.2">
<h3><span class="header-section-number">13.3.2</span> Scaling<a href="machine-learning.html#scaling-1" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Remember, kNN analysis is like cluster analysis in that it is based on the <em>distance</em> between observations. kNN seeks to identify <span class="math inline">\(k\)</span> neighbors that are most similar to the individual for whom we are trying to predict the missing value.</p>
<p>Variables with a greater range of values will be more heavily weighted in distance calculations, giving them excessive influence in how we measure distance (similarity).</p>
<p>The solution to this is to scale the data the same as we did for cluster analysis.</p>
</div>
<div id="k-nearest-neighbor-animation" class="section level3 hasAnchor" number="13.3.3">
<h3><span class="header-section-number">13.3.3</span> k-Nearest-Neighbor Animation<a href="machine-learning.html#k-nearest-neighbor-animation" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>In kNN analysis, for each individual for which we are trying to make a prediction, the distance to each other individual is calculated. As a reminder, the Euclidean distance between points <span class="math inline">\(p\)</span> and <span class="math inline">\(q\)</span> is calculated as:</p>
<p><span class="math display">\[dist(p,q) = \sqrt{(p_1 - q_1)^2 + (p_2 - q_2)^2 ... (p_n - q_n)^2} \]</span></p>
<p>The <span class="math inline">\(k\)</span> nearest (most similar) observations are then identified. What happens next depends on whether the measure we are trying to predict is qualitative or quantitative. Are we trying to predict a drainage category? Whether the climate is “hot” vs “cold”? In these cases, the algorithm will identify the value that occurs most frequently among those neighbors. This statistic is called the <em>mode</em>. The mode will then become the predicted value for the original individual. Let’s demonstrate this below.</p>
<p>In this example, corn yield is predicted based on cumulative growing degree days and percent silt. Because it is easier to illustrate qualitative predictions, corn yield has been categorized within four ranges: 0-100 (circle), 100-120 (triangle), 120-140 (cross), and 140+ (“x”).</p>
<p>The known yields, referred to as the training set, are colored blue. The individuals for which we are are trying to predict yield are represented by red question marks. Although the axes names are ambiguous, each known point is plotted according to its cumulative growing degree days (cum_gdd) and silt content.</p>
<p>This example uses <span class="math inline">\(k\)</span>=5. That is, the missing values of individuals will be calculated based on the most frequent yield classification of their 5 nearest neighbors. There are three points, indicated by red question marks for which we are going to try to pick the value. We start with the leftmost question mark.</p>
<p><img src="data-unit-13/knn_frames/frame_00_delay-1s.jpg" /></p>
<p>For that point, we measure the distance to every other point:</p>
<p><img src="data-unit-13/knn_frames/frame_01_delay-1s.jpg" /></p>
<p>And identify the nearest 5 points (neighbors). Of the five nearest neighbors, there is one square (yield up to 100 bushels), two circles (indicating yields between 100 and 120 bushels) and two triangles (yields between 120 and 140 bushels).</p>
<p><img src="data-unit-13/knn_frames/frame_02_delay-1s.jpg" /></p>
<p>There are two each circles and triangles, so R randomly selects from those two values. The missing value is classified as a circle.</p>
<p><img src="data-unit-13/knn_frames/frame_03_delay-1s.jpg" /></p>
<p>Let’s repeat this process for the next missing value:</p>
<p><img src="data-unit-13/knn_frames/frame_04_delay-1s.jpg" /></p>
<p>For this point, the five nearest neighbors are again identified.</p>
<p><img src="data-unit-13/knn_frames/frame_06_delay-1s.jpg" /></p>
<p>In this case there are three triangles and two circles, so the missing value is estimated to be a triangle.</p>
<p><img src="data-unit-13/knn_frames/frame_06_delay-1s.jpg" /></p>
<p>For the last point, the nearest 5 neighbors include two circles, two triangles, and one diamond.</p>
<p><img src="data-unit-13/knn_frames/frame_10_delay-1s.jpg" /></p>
<p>R randomly chooses between the triangle and circle – again, the circle wins.</p>
<p><img src="data-unit-13/knn_frames/frame_11_delay-1s.jpg" /></p>
</div>
<div id="choosing-k" class="section level3 hasAnchor" number="13.3.4">
<h3><span class="header-section-number">13.3.4</span> Choosing <span class="math inline">\(k\)</span><a href="machine-learning.html#choosing-k" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>In the example above, a value of 5 chosen arbitrarily to keep the illustation from becomming too muddled. How should we choose <span class="math inline">\(k\)</span> in actual analyses?</p>
<p>Your first instinct might be to select a large <span class="math inline">\(k\)</span>. Why? Because we have learned throughout this course that larger sample sizes reduce the influences of outliers on our predictions. By choosing a larger k, we reduce the likelihood an extreme value (a bad neighbor?) will unduly skew our prediction. We reduce the potential for bias.</p>
<p>The flip side of this is: the more neighbors we include, the less “near” they will be to the individual for which we are trying to make a prediction. We reduce the influence of outlier, but at the same time predict a value that is closer to the population mean, rather than the true missing value. In other words, a larger <span class="math inline">\(k\)</span> also reduces our precision.</p>
<p>A general rule of thumb is to choose a <span class="math inline">\(k\)</span> equal to the square root of the total number of individuals in the population. We had 50 individuals in the population above, so we could have set our <span class="math inline">\(k\)</span> equal to 7.</p>
</div>
<div id="model-cross-validation" class="section level3 hasAnchor" number="13.3.5">
<h3><span class="header-section-number">13.3.5</span> Model Cross-Validation<a href="machine-learning.html#model-cross-validation" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>When we fit a kNN model, we should also cross-validate it. In cross-validation, the data are portioned into two groups: a training group and a testing group. The model is fit using the training group. It is then used to make predictions for the testing group.</p>
<p>The figures below illustrate a common cross-validation method, k-fold cross validation. In this method, the data are divided into k number of groups. In this example, k=10, meaning the data are divided into 10 groups. Nine of the groups (in blue) is are combined to train the model. The tenth group (in red) is the testing group, and used to validate the fit. The validation process goes through 10 rounds.</p>
<p>In the first round, Groups 2 through 9 are used to train the model. The fit of the model is then tested by how closely it can predict the observed values in Group 1.</p>
<p><img src="data-unit-13/cv/frame_00_delay-1s.jpg" /></p>
<p>In the first round, Groups 1, 3, 4, 5, 6, 7, 8, 9, and 10 are used to train the model. The fit of the model is then tested by how closely it can predict the observed values in Group 2.</p>
<p><img src="data-unit-13/cv/frame_01_delay-1s.jpg" /></p>
<p>In the third round, Groups 1, 2, 4, 5, 6, 7, 8, 9, and 10 are used to train the model. The fit of the model is then tested by how closely it can predict the observed values in Group 3.</p>
<p><img src="data-unit-13/cv/frame_02_delay-1s.jpg" /></p>
<p>This continues until each group has been used to train and test the model.</p>
<p><img src="data-unit-13/cv/frame_09_delay-1s.jpg" /></p>
<p>A linear regression of the predicted test values on the actual test values is then conducted and the regression coefficient, <span class="math inline">\(R^2\)</span>, is calculated. The Root Mean Square Error is also calculated during the regression. You will recall the Root MSE is a measure of the standard deviation of the difference between the predicted and actual values.</p>
</div>
<div id="yield-prediction-with-nearest-neighbor-analysis" class="section level3 hasAnchor" number="13.3.6">
<h3><span class="header-section-number">13.3.6</span> Yield Prediction with Nearest Neighbor Analysis<a href="machine-learning.html#yield-prediction-with-nearest-neighbor-analysis" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>As mentioned above, nearest neighbor analysis also works with qualitative data. In this case, the values of the nearest neighbors are averaged for the variable of interest. In R, there are various functions for running a nearest-neighbor analysis. The output below was generated using the <em>caret</em> package, a very powerful tool which fits and cross-validates models at once.</p>
<div class="sourceCode" id="cb986"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb986-1"><a href="machine-learning.html#cb986-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(caret)</span>
<span id="cb986-2"><a href="machine-learning.html#cb986-2" aria-hidden="true" tabindex="-1"></a>ctrl <span class="ot">=</span> <span class="fu">trainControl</span>(<span class="at">method=</span><span class="st">&quot;repeatedcv&quot;</span>, <span class="at">number=</span><span class="dv">10</span>, <span class="at">repeats =</span> <span class="dv">1</span>)</span>
<span id="cb986-3"><a href="machine-learning.html#cb986-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb986-4"><a href="machine-learning.html#cb986-4" aria-hidden="true" tabindex="-1"></a><span class="co">#knn</span></span>
<span id="cb986-5"><a href="machine-learning.html#cb986-5" aria-hidden="true" tabindex="-1"></a>knnFit <span class="ot">=</span> <span class="fu">train</span>(corn <span class="sc">~</span> .,</span>
<span id="cb986-6"><a href="machine-learning.html#cb986-6" aria-hidden="true" tabindex="-1"></a>                <span class="at">data =</span> trainData,</span>
<span id="cb986-7"><a href="machine-learning.html#cb986-7" aria-hidden="true" tabindex="-1"></a>                <span class="at">method =</span> <span class="st">&quot;knn&quot;</span>,</span>
<span id="cb986-8"><a href="machine-learning.html#cb986-8" aria-hidden="true" tabindex="-1"></a>                <span class="at">preProc =</span> <span class="fu">c</span>(<span class="st">&quot;center&quot;</span>, <span class="st">&quot;scale&quot;</span>),</span>
<span id="cb986-9"><a href="machine-learning.html#cb986-9" aria-hidden="true" tabindex="-1"></a>                <span class="at">trControl =</span> ctrl)</span>
<span id="cb986-10"><a href="machine-learning.html#cb986-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb986-11"><a href="machine-learning.html#cb986-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb986-12"><a href="machine-learning.html#cb986-12" aria-hidden="true" tabindex="-1"></a>knnFit</span></code></pre></div>
<pre><code>## k-Nearest Neighbors 
## 
## 665 samples
##   7 predictor
## 
## Pre-processing: centered (7), scaled (7) 
## Resampling: Cross-Validated (10 fold, repeated 1 times) 
## Summary of sample sizes: 599, 599, 598, 599, 597, 598, ... 
## Resampling results across tuning parameters:
## 
##   k  RMSE      Rsquared   MAE     
##   5  11.04766  0.8129430  7.996958
##   7  10.99521  0.8177150  8.001320
##   9  11.07555  0.8157225  8.147071
## 
## RMSE was used to select the optimal model using the smallest value.
## The final value used for the model was k = 7.</code></pre>
<p>In the above output, the following are important:</p>
<ul>
<li>Resampling: this line tells us how the model was cross-validated (10-fold cross validation)</li>
<li>Identification of the final value used for the model: Rememember when we discussed how to select the appropriate <span class="math inline">\(k\)</span> (number of neighbors)? R compares multiple values of <span class="math inline">\(k\)</span>. In this case, it identifies <span class="math inline">\(k\)</span>=7 as optimum for model performance.<br />
</li>
<li>Model statistics table. Statistics for three levels of <span class="math inline">\(k\)</span> are given: 5, 7, and 9. RMSE is the root means square error, Rsquared is <span class="math inline">\(R^2\)</span>, the regression coefficient, and MAE is Mean Absolute Error (an alternative calculation of error that does not involve squaring the error).</li>
</ul>
<p>Looking at the table results for k=7, we see the model had an <span class="math inline">\(R^2\)</span> of almost 0.81. This is pretty impressive, given that were predicting mean county corn yield using relatively few measures of soil and environment. The RMSE is about 11.1. Using this with our Z-distribution, we can predict there is a 95% chance the actual corn yield will be within <span class="math inline">\(1.96 \times11.1=21.8\)</span> bushels of the predicted yield.</p>
<p>In this example, as you might have suspected, we did know the corn yield in the counties where it was “missing”; we just selected those counties at random and used them to show how k-nearest-neighbors worked.</p>
<p>So let’s use the our nearest-neighbor model to predict the yields in those “missing” counties and compare them to the actual observed yields. We can use a simple linear regression model for this, where:</p>
<p><span class="math display">\[ Predicted \ \ Yield = \alpha \ \ + \ \beta\ \cdot Actual \ \ Yield \]</span></p>
<p>Below, the predicted yields (y-axis) are regressed against the actual yields (x-axis).</p>
<div class="sourceCode" id="cb988"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb988-1"><a href="machine-learning.html#cb988-1" aria-hidden="true" tabindex="-1"></a>testData<span class="sc">$</span>corn_pred <span class="ot">=</span> <span class="fu">predict</span>(knnFit, testData)</span>
<span id="cb988-2"><a href="machine-learning.html#cb988-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb988-3"><a href="machine-learning.html#cb988-3" aria-hidden="true" tabindex="-1"></a>testData <span class="sc">%&gt;%</span></span>
<span id="cb988-4"><a href="machine-learning.html#cb988-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x=</span>corn, <span class="at">y=</span>corn_pred)) <span class="sc">+</span></span>
<span id="cb988-5"><a href="machine-learning.html#cb988-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>() <span class="sc">+</span></span>
<span id="cb988-6"><a href="machine-learning.html#cb988-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_smooth</span>(<span class="at">method =</span> <span class="st">&quot;lm&quot;</span>) <span class="sc">+</span></span>
<span id="cb988-7"><a href="machine-learning.html#cb988-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">x=</span><span class="st">&quot;Actual Yield (bu)&quot;</span>, <span class="at">y=</span><span class="st">&quot;Predicted Yield (bu)&quot;</span>)</span></code></pre></div>
<pre><code>## `geom_smooth()` using formula &#39;y ~ x&#39;</code></pre>
<p><img src="13-Machine-Learning_files/figure-html/unnamed-chunk-17-1.png" width="672" /></p>
<p>We can see the predicted and actual yields are strongly correlated, as evidenced by their linear distribution and proximity to the regression line. The regression model statistics are shown below.</p>
<div class="sourceCode" id="cb990"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb990-1"><a href="machine-learning.html#cb990-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(broom)</span></code></pre></div>
<pre><code>## Warning: package &#39;broom&#39; was built under R version 4.1.3</code></pre>
<div class="sourceCode" id="cb992"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb992-1"><a href="machine-learning.html#cb992-1" aria-hidden="true" tabindex="-1"></a>model <span class="ot">=</span> <span class="fu">lm</span>(corn_pred<span class="sc">~</span>corn, <span class="at">data=</span>testData)</span>
<span id="cb992-2"><a href="machine-learning.html#cb992-2" aria-hidden="true" tabindex="-1"></a><span class="fu">glance</span>(model) <span class="sc">%&gt;%</span></span>
<span id="cb992-3"><a href="machine-learning.html#cb992-3" aria-hidden="true" tabindex="-1"></a>  dplyr<span class="sc">::</span><span class="fu">select</span>(r.squared, sigma, p.value) <span class="sc">%&gt;%</span></span>
<span id="cb992-4"><a href="machine-learning.html#cb992-4" aria-hidden="true" tabindex="-1"></a>  kableExtra<span class="sc">::</span><span class="fu">kbl</span>()</span></code></pre></div>
<table>
<thead>
<tr>
<th style="text-align:right;">
r.squared
</th>
<th style="text-align:right;">
sigma
</th>
<th style="text-align:right;">
p.value
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:right;">
0.785811
</td>
<td style="text-align:right;">
10.35285
</td>
<td style="text-align:right;">
0
</td>
</tr>
</tbody>
</table>
<p>Above are select statistics for our model. We see the r.squared, about 0.83 is even better greater than in our cross-validation. Sigma, the standard deviation, is smaller than in the validation, barely 9 bushels. Finally, we see the significance of our model is very, very small, meaning the relationship between predicted and actual yield was very strong.</p>
<div class="sourceCode" id="cb993"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb993-1"><a href="machine-learning.html#cb993-1" aria-hidden="true" tabindex="-1"></a>complete_dataset <span class="ot">=</span> testData <span class="sc">%&gt;%</span></span>
<span id="cb993-2"><a href="machine-learning.html#cb993-2" aria-hidden="true" tabindex="-1"></a>  dplyr<span class="sc">::</span><span class="fu">select</span>(<span class="sc">-</span>corn) <span class="sc">%&gt;%</span></span>
<span id="cb993-3"><a href="machine-learning.html#cb993-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">rename</span>(<span class="at">corn=</span>corn_pred) <span class="sc">%&gt;%</span></span>
<span id="cb993-4"><a href="machine-learning.html#cb993-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">rbind</span>(trainPartition)</span></code></pre></div>
<p>Finally, we can fill in the missing county yields in the map with which we started this section. We see the predicted yields for the missing county seem to be in line with their surrounding counties; there are no islands of extremely high or extremely low yields. You can see the yield of each county by clicking on it.</p>
<div class="sourceCode" id="cb994"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb994-1"><a href="machine-learning.html#cb994-1" aria-hidden="true" tabindex="-1"></a>pal_corn <span class="ot">=</span> <span class="fu">colorBin</span>(<span class="st">&quot;RdYlGn&quot;</span>, <span class="at">bins=</span><span class="dv">5</span>, complete_dataset<span class="sc">$</span>corn)</span>
<span id="cb994-2"><a href="machine-learning.html#cb994-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb994-3"><a href="machine-learning.html#cb994-3" aria-hidden="true" tabindex="-1"></a>m <span class="ot">&lt;-</span> complete_dataset <span class="sc">%&gt;%</span></span>
<span id="cb994-4"><a href="machine-learning.html#cb994-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">leaflet</span>() <span class="sc">%&gt;%</span></span>
<span id="cb994-5"><a href="machine-learning.html#cb994-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">addTiles</span>() <span class="sc">%&gt;%</span></span>
<span id="cb994-6"><a href="machine-learning.html#cb994-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">addPolygons</span>(</span>
<span id="cb994-7"><a href="machine-learning.html#cb994-7" aria-hidden="true" tabindex="-1"></a>    <span class="at">fillColor =</span> <span class="sc">~</span><span class="fu">pal_corn</span>(corn),</span>
<span id="cb994-8"><a href="machine-learning.html#cb994-8" aria-hidden="true" tabindex="-1"></a>    <span class="at">fillOpacity =</span> <span class="fl">0.8</span>,</span>
<span id="cb994-9"><a href="machine-learning.html#cb994-9" aria-hidden="true" tabindex="-1"></a>    <span class="at">weight=</span><span class="dv">1</span>,</span>
<span id="cb994-10"><a href="machine-learning.html#cb994-10" aria-hidden="true" tabindex="-1"></a>    <span class="at">color =</span> <span class="st">&quot;black&quot;</span>,</span>
<span id="cb994-11"><a href="machine-learning.html#cb994-11" aria-hidden="true" tabindex="-1"></a>    <span class="at">popup =</span> <span class="fu">paste</span>(<span class="st">&quot;yield:&quot;</span>, <span class="fu">as.character</span>(<span class="fu">round</span>(complete_dataset<span class="sc">$</span>corn,<span class="dv">1</span>)))</span>
<span id="cb994-12"><a href="machine-learning.html#cb994-12" aria-hidden="true" tabindex="-1"></a>  ) <span class="sc">%&gt;%</span></span>
<span id="cb994-13"><a href="machine-learning.html#cb994-13" aria-hidden="true" tabindex="-1"></a>  <span class="fu">addLegend</span>(<span class="at">pal =</span> pal_corn,</span>
<span id="cb994-14"><a href="machine-learning.html#cb994-14" aria-hidden="true" tabindex="-1"></a>            <span class="at">values =</span> <span class="sc">~</span>corn)</span>
<span id="cb994-15"><a href="machine-learning.html#cb994-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb994-16"><a href="machine-learning.html#cb994-16" aria-hidden="true" tabindex="-1"></a><span class="co">#above code won&#39;t knit -- created static image for markdown</span></span>
<span id="cb994-17"><a href="machine-learning.html#cb994-17" aria-hidden="true" tabindex="-1"></a><span class="fu">saveWidget</span>(m, <span class="st">&quot;temp.html&quot;</span>, <span class="at">selfcontained =</span> <span class="cn">FALSE</span>)</span>
<span id="cb994-18"><a href="machine-learning.html#cb994-18" aria-hidden="true" tabindex="-1"></a><span class="fu">webshot</span>(<span class="st">&quot;temp.html&quot;</span>, <span class="at">cliprect =</span> <span class="st">&quot;viewport&quot;</span>)</span></code></pre></div>
<p><img src="13-Machine-Learning_files/figure-html/unnamed-chunk-20-1.png" width="672" /></p>
</div>
</div>
<div id="classification-trees" class="section level2 hasAnchor" number="13.4">
<h2><span class="header-section-number">13.4</span> Classification Trees<a href="machine-learning.html#classification-trees" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>The last data science tool we will learn in this unit the classification tree. The classification tree uses a “divide and conquer” or sorting approach to predict the value of a response variable, based on the levels of its predictor variables.</p>
<p>The best way to understand how a classification tree works is to play the game Plinko. In Plinko, a player drops disc at the top of a table studded with pins.</p>
<p><img src="data-unit-13/plinko/frame_00_delay-0.14s.jpg" /></p>
<p>The disc falls down the table, bouncing to one side or the other of each successive pin.</p>
<p><img src="data-unit-13/plinko/frame_03_delay-0.14s.jpg" /></p>
<p><img src="data-unit-13/plinko/frame_11_delay-0.14s.jpg" /></p>
<p><img src="data-unit-13/plinko/frame_17_delay-0.14s.jpg" /></p>
<p><img src="data-unit-13/plinko/frame_30_delay-0.14s.jpg" /></p>
<p>The bin determines what fabulous prize the player may have won.</p>
<p><img src="data-unit-13/plinko/frame_44_delay-0.14s.jpg" /></p>
<p>Each level of the decision tree can be thought of as having pins. Each pin is a division point along variable of interest. If an observation has a value for that variable which is greater than the division point, it is classified one way and moves to the next pin. If less, it moves the other way where it encounters a different pin.</p>
<div id="features" class="section level3 hasAnchor" number="13.4.1">
<h3><span class="header-section-number">13.4.1</span> Features<a href="machine-learning.html#features" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Classification trees are different from cluster and nearest neighbor analyses in that they identify the relative imporance of <em>features</em>. Features include every predictor variable in the dataset. The higher a feature occurs in a classification tree, the more important they are are in predicting the final value.</p>
<p>Classification trees therefore shed a unique and valuable insight into the importance of variables, distinct from all other analyses we have learned. They provide insight into <em>feature importance</em>, not just their significance as a predictor.</p>
</div>
<div id="quantitative-categorical-data" class="section level3 hasAnchor" number="13.4.2">
<h3><span class="header-section-number">13.4.2</span> Quantitative (Categorical) Data<a href="machine-learning.html#quantitative-categorical-data" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>A classification tree predicts the category of the response variable. We will start with yield categories, just as we did with the nearest neighbors section, because the results are intuitively easier to visualize and understand.</p>
<p>The classification process begins with the selection of a root. This is the feature that, when split, is the best single predictor of the response variable. The algorithm not only selects the root; it also determines where within the range (the threshold) of that feature to divide the populations.</p>
<p>What is the criterion for this threshold? It is the value that divides the population into two groups in which the individuals within the group are as similar as possible.</p>
<p>These results are displayed as a tree; thus, the “classification tree”. The results of our first split are shown below:</p>
<div class="sourceCode" id="cb995"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb995-1"><a href="machine-learning.html#cb995-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(party)</span></code></pre></div>
<pre><code>## Loading required package: grid</code></pre>
<pre><code>## Loading required package: mvtnorm</code></pre>
<pre><code>## Loading required package: modeltools</code></pre>
<pre><code>## Loading required package: stats4</code></pre>
<pre><code>## Loading required package: strucchange</code></pre>
<pre><code>## Loading required package: zoo</code></pre>
<pre><code>## 
## Attaching package: &#39;zoo&#39;</code></pre>
<pre><code>## The following objects are masked from &#39;package:base&#39;:
## 
##     as.Date, as.Date.numeric</code></pre>
<pre><code>## Loading required package: sandwich</code></pre>
<pre><code>## 
## Attaching package: &#39;strucchange&#39;</code></pre>
<pre><code>## The following object is masked from &#39;package:stringr&#39;:
## 
##     boundary</code></pre>
<div class="sourceCode" id="cb1007"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1007-1"><a href="machine-learning.html#cb1007-1" aria-hidden="true" tabindex="-1"></a>x <span class="ot">=</span> trainData <span class="sc">%&gt;%</span></span>
<span id="cb1007-2"><a href="machine-learning.html#cb1007-2" aria-hidden="true" tabindex="-1"></a>  dplyr<span class="sc">::</span><span class="fu">select</span>(cum_gdd, silt, corn)</span>
<span id="cb1007-3"><a href="machine-learning.html#cb1007-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1007-4"><a href="machine-learning.html#cb1007-4" aria-hidden="true" tabindex="-1"></a>classes <span class="ot">=</span> <span class="fu">cut</span>(x<span class="sc">$</span>corn, <span class="at">breaks=</span><span class="fu">c</span>(<span class="sc">-</span><span class="cn">Inf</span>, <span class="dv">100</span>, <span class="dv">120</span>, <span class="dv">140</span>, <span class="cn">Inf</span>))</span>
<span id="cb1007-5"><a href="machine-learning.html#cb1007-5" aria-hidden="true" tabindex="-1"></a>x <span class="ot">=</span> x <span class="sc">%&gt;%</span></span>
<span id="cb1007-6"><a href="machine-learning.html#cb1007-6" aria-hidden="true" tabindex="-1"></a>  dplyr<span class="sc">::</span><span class="fu">select</span>(<span class="sc">-</span>corn)</span>
<span id="cb1007-7"><a href="machine-learning.html#cb1007-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1007-8"><a href="machine-learning.html#cb1007-8" aria-hidden="true" tabindex="-1"></a>fit <span class="ot">=</span> <span class="fu">ctree</span>(classes <span class="sc">~</span>., <span class="at">data=</span>x,</span>
<span id="cb1007-9"><a href="machine-learning.html#cb1007-9" aria-hidden="true" tabindex="-1"></a>            <span class="at">controls =</span> <span class="fu">ctree_control</span>(<span class="at">maxdepth =</span> <span class="dv">1</span>))</span>
<span id="cb1007-10"><a href="machine-learning.html#cb1007-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1007-11"><a href="machine-learning.html#cb1007-11" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(fit)</span></code></pre></div>
<p><img src="13-Machine-Learning_files/figure-html/unnamed-chunk-21-1.png" width="672" /></p>
<p>Cumulative growing degree days (cum_gdd) was selected as the root, and the threshold or split value was approximately 2198 GDD. Counties where the cumulative GDD was less than this threshold tended to have yields in the lower two classes; above this threshold, counties tended to have yields in the greater two classes.</p>
<p>This particular algorithm also calculates a p-value that the distribution of the counts among the yield classes is different for the two groups. We see the difference betwen the two groups (greater vs less than 2198 GDD) is highly significant, with a p-value &lt; 0.001.</p>
<p>Next, let’s add another level to our tree. The two subgroups will each be split into two new subgroups.</p>
<div class="sourceCode" id="cb1008"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1008-1"><a href="machine-learning.html#cb1008-1" aria-hidden="true" tabindex="-1"></a>fit <span class="ot">=</span> <span class="fu">ctree</span>(classes <span class="sc">~</span>., <span class="at">data=</span>x,</span>
<span id="cb1008-2"><a href="machine-learning.html#cb1008-2" aria-hidden="true" tabindex="-1"></a>            <span class="at">controls =</span> <span class="fu">ctree_control</span>(<span class="at">maxdepth =</span> <span class="dv">2</span>))</span>
<span id="cb1008-3"><a href="machine-learning.html#cb1008-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1008-4"><a href="machine-learning.html#cb1008-4" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(fit)</span></code></pre></div>
<p><img src="13-Machine-Learning_files/figure-html/unnamed-chunk-22-1.png" width="672" /></p>
<p>Our tree now has three parts. The root was discussed before. At the bottom of the tree, we have four nodes: Node 3, Node 4, Node 6, and Node 7. These represent the final groupings of our data.</p>
<p>In the middle, we have two branches, labelled as branches 2 and 5 of our tree. Each branch is indicated by an oval. The branch number is in a box at the top of the oval.</p>
<p>Counties that had cum_gdd &lt; 2198 were divided at branch 2 by cum_gdd, this time into counties with less than 1881 cum_gdd and counties with more than 1881 cum_gdd. Almost all counties with less than 1881 cum_gdd had yields in the lowest class. Above 1881 cum_gdd, most yields were still in the lowest class, but some were in the second lowest class.</p>
<p>Counties with greater than 2198 cum_gdd were divided at branch 5 by soil percent silt. Counties with soils that with less than 28.2 percent slit had yields that were mostly in the middle two yield classes. Above 28.2 percent silt, most yields were in the upper two classes.</p>
<p>The beauty of the regression tree is we are left with a distinct set of rules for predicting the outcome of our response variable. This lends itself nicely to explaining our proeduction. Why do we predict a county yield will be in one of the two highest classes? Because the county has cumulative GDDs greater than 2198 and a soil that has a percent silt greater than 28.2</p>
<p>The algorithm will continue to add depths to the tree until the nodes are as homogeneous as possible. The tree, however, will then become too complex to visualize. Below we have three levels to our tree – we can see how difficult it is to read.</p>
<div class="sourceCode" id="cb1009"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1009-1"><a href="machine-learning.html#cb1009-1" aria-hidden="true" tabindex="-1"></a>fit <span class="ot">=</span> <span class="fu">ctree</span>(classes <span class="sc">~</span>., <span class="at">data=</span>x,</span>
<span id="cb1009-2"><a href="machine-learning.html#cb1009-2" aria-hidden="true" tabindex="-1"></a>            <span class="at">controls =</span> <span class="fu">ctree_control</span>(<span class="at">maxdepth =</span> <span class="dv">3</span>))</span>
<span id="cb1009-3"><a href="machine-learning.html#cb1009-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1009-4"><a href="machine-learning.html#cb1009-4" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(fit)</span></code></pre></div>
<p><img src="13-Machine-Learning_files/figure-html/unnamed-chunk-23-1.png" width="672" /></p>
</div>
<div id="quanatitiative-continuous-data" class="section level3 hasAnchor" number="13.4.3">
<h3><span class="header-section-number">13.4.3</span> Quanatitiative (Continuous) Data<a href="machine-learning.html#quanatitiative-continuous-data" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The same process above can be used with quantitative data – in this example, the actual yields. When we use yield, the root of our tree is not cumulative growing degree days, but total precipitation. Instead of a bar plot showing us the number of individuals in each class, we get a boxplot showing the distribution of the observations in each node. When we work with quantitative data, R identifies the feature and split that minimizes the variance in each node.</p>
<div class="sourceCode" id="cb1010"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1010-1"><a href="machine-learning.html#cb1010-1" aria-hidden="true" tabindex="-1"></a>fit <span class="ot">=</span> <span class="fu">ctree</span>(corn <span class="sc">~</span>., <span class="at">data=</span>trainData,</span>
<span id="cb1010-2"><a href="machine-learning.html#cb1010-2" aria-hidden="true" tabindex="-1"></a>            <span class="at">controls =</span> <span class="fu">ctree_control</span>(<span class="at">maxdepth =</span> <span class="dv">1</span>))</span>
<span id="cb1010-3"><a href="machine-learning.html#cb1010-3" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(fit)</span></code></pre></div>
<p><img src="13-Machine-Learning_files/figure-html/unnamed-chunk-24-1.png" width="672" /></p>
<p>When we add a second level to our tree, we see that observations where precipitation was less than or equal to about 446.5 mm/season, the data was next split by cum_gdd. Counties with a cum_gdd les than about 2198 had lower yields than counties with a cum_gdd &gt; 2198.</p>
<p>Where ppt was greater than 446 mm per season, individuals were next split by water holding capacity (whc). Counties where whc was greater than about 28.8 cm/cm soil had greater yield than counties with less water holding capacity.</p>
<div class="sourceCode" id="cb1011"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1011-1"><a href="machine-learning.html#cb1011-1" aria-hidden="true" tabindex="-1"></a>fit <span class="ot">&lt;-</span> <span class="fu">ctree</span>(corn <span class="sc">~</span> ., <span class="at">data=</span>trainData,</span>
<span id="cb1011-2"><a href="machine-learning.html#cb1011-2" aria-hidden="true" tabindex="-1"></a>             <span class="at">controls =</span> <span class="fu">ctree_control</span>(<span class="at">maxdepth =</span> <span class="dv">2</span>))</span>
<span id="cb1011-3"><a href="machine-learning.html#cb1011-3" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(fit)</span></code></pre></div>
<p><img src="13-Machine-Learning_files/figure-html/unnamed-chunk-25-1.png" width="672" /></p>
<p>Like before, we can continue adding branches, although our virtual interpretation of the data will become more challenging.</p>
</div>
<div id="overfitting-1" class="section level3 hasAnchor" number="13.4.4">
<h3><span class="header-section-number">13.4.4</span> Overfitting<a href="machine-learning.html#overfitting-1" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>In nearest-neighbors analysis, we learned the size of k (the number of neighbors) was a tradeoff: too few neighbors, and the risk of an outlier skewing the prediction increased. Too many neighbors, and the model prediction approached the mean of the entire population. The secret was to solve for the optimum value of k.</p>
<p>There is a similar tradeoff with classification trees. We can continue adding layers to our decision tree until only one observation is left in each node. At that point, we will perfectly predict each individual in the population. Sounds great, huh?</p>
<p>Yeah, but what about when you use the classification model (with its exact set of rules for the training population) on a new population? It is likely the new population will follow the more general rules, defined by the population splits closer to the root of the tree. It is less likely the population will follow the specific rules that define the nodes and the branches immediately above.</p>
<p>Just as a bush around your house can become messing and overgrown, so can a classification tree become so branched it loses its value. The solution in each case is to prune, so that the bush regains its shape, and the classification tree approaches a useful combination of general application and accurate predictions.</p>
</div>
<div id="cross-validation-2" class="section level3 hasAnchor" number="13.4.5">
<h3><span class="header-section-number">13.4.5</span> Cross Validation<a href="machine-learning.html#cross-validation-2" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Just as with nearest-neighbors, the answer to overfitting classification trees is cross-validation. Using the same cross-validation technique (10-fold cross-validation) we used above, let’s fit our classification tree.</p>
<div class="sourceCode" id="cb1012"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1012-1"><a href="machine-learning.html#cb1012-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(caret)</span>
<span id="cb1012-2"><a href="machine-learning.html#cb1012-2" aria-hidden="true" tabindex="-1"></a>ctrl <span class="ot">=</span> <span class="fu">trainControl</span>(<span class="at">method=</span><span class="st">&quot;repeatedcv&quot;</span>, <span class="at">number=</span><span class="dv">10</span>, <span class="at">repeats =</span> <span class="dv">1</span>)</span>
<span id="cb1012-3"><a href="machine-learning.html#cb1012-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1012-4"><a href="machine-learning.html#cb1012-4" aria-hidden="true" tabindex="-1"></a><span class="co">#knn</span></span>
<span id="cb1012-5"><a href="machine-learning.html#cb1012-5" aria-hidden="true" tabindex="-1"></a>partyFit <span class="ot">=</span> <span class="fu">train</span>(corn <span class="sc">~</span> .,</span>
<span id="cb1012-6"><a href="machine-learning.html#cb1012-6" aria-hidden="true" tabindex="-1"></a>                <span class="at">data =</span> trainData,</span>
<span id="cb1012-7"><a href="machine-learning.html#cb1012-7" aria-hidden="true" tabindex="-1"></a>                <span class="at">method =</span> <span class="st">&quot;ctree&quot;</span>,</span>
<span id="cb1012-8"><a href="machine-learning.html#cb1012-8" aria-hidden="true" tabindex="-1"></a>                <span class="at">trControl =</span> ctrl)</span>
<span id="cb1012-9"><a href="machine-learning.html#cb1012-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1012-10"><a href="machine-learning.html#cb1012-10" aria-hidden="true" tabindex="-1"></a>partyFit</span></code></pre></div>
<pre><code>## Conditional Inference Tree 
## 
## 665 samples
##   7 predictor
## 
## No pre-processing
## Resampling: Cross-Validated (10 fold, repeated 1 times) 
## Summary of sample sizes: 598, 599, 599, 599, 598, 598, ... 
## Resampling results across tuning parameters:
## 
##   mincriterion  RMSE      Rsquared   MAE      
##   0.01          13.05396  0.7412480   9.689862
##   0.50          13.20743  0.7342159   9.974611
##   0.99          15.13028  0.6567629  11.933852
## 
## RMSE was used to select the optimal model using the smallest value.
## The final value used for the model was mincriterion = 0.01.</code></pre>
<p>The output looks very similar to that for nearest neighbor, except instead of <span class="math inline">\(k\)</span> we see a statistic called mincriterion. The mincriterion specifies the maxiumum p-value for any branch to be included in the final model.</p>
<p>As we discussed above, the ability of the classification tree to fit new data decreases as the number of branches and nodes increases. At the same time, we want to include enough branches in the model so that we are accurate in our classifications.</p>
<p>In the output above, the mincriterion was 0.01, meaning the p-value must be no greater than 0.01. This gave us an Rsquared of about 0.745 and and Root MSE of about 13.0.</p>
<p>As with the <span class="math inline">\(k\)</span>-Nearest-Neighbors analysis, we can compare the predicted yields for the counties dropped from our model to their actual values.</p>
<div class="sourceCode" id="cb1014"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1014-1"><a href="machine-learning.html#cb1014-1" aria-hidden="true" tabindex="-1"></a>testData<span class="sc">$</span>corn_pred <span class="ot">=</span> <span class="fu">predict</span>(partyFit, testData)</span>
<span id="cb1014-2"><a href="machine-learning.html#cb1014-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1014-3"><a href="machine-learning.html#cb1014-3" aria-hidden="true" tabindex="-1"></a>testData <span class="sc">%&gt;%</span></span>
<span id="cb1014-4"><a href="machine-learning.html#cb1014-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x=</span>corn, <span class="at">y=</span>corn_pred)) <span class="sc">+</span></span>
<span id="cb1014-5"><a href="machine-learning.html#cb1014-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>() <span class="sc">+</span></span>
<span id="cb1014-6"><a href="machine-learning.html#cb1014-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_smooth</span>(<span class="at">method =</span> <span class="st">&quot;lm&quot;</span>) <span class="sc">+</span></span>
<span id="cb1014-7"><a href="machine-learning.html#cb1014-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">x=</span><span class="st">&quot;Actual Yield (bu)&quot;</span>, <span class="at">y=</span><span class="st">&quot;Predicted Yield (bu)&quot;</span>)</span></code></pre></div>
<pre><code>## `geom_smooth()` using formula &#39;y ~ x&#39;</code></pre>
<p><img src="13-Machine-Learning_files/figure-html/unnamed-chunk-27-1.png" width="672" /></p>
<div class="sourceCode" id="cb1016"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1016-1"><a href="machine-learning.html#cb1016-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(broom)</span>
<span id="cb1016-2"><a href="machine-learning.html#cb1016-2" aria-hidden="true" tabindex="-1"></a>model <span class="ot">=</span> <span class="fu">lm</span>(corn_pred<span class="sc">~</span>corn, <span class="at">data=</span>testData)</span>
<span id="cb1016-3"><a href="machine-learning.html#cb1016-3" aria-hidden="true" tabindex="-1"></a><span class="fu">glance</span>(model) <span class="sc">%&gt;%</span></span>
<span id="cb1016-4"><a href="machine-learning.html#cb1016-4" aria-hidden="true" tabindex="-1"></a>  dplyr<span class="sc">::</span><span class="fu">select</span>(r.squared, sigma, p.value) </span></code></pre></div>
<pre><code>## # A tibble: 1 x 3
##   r.squared sigma  p.value
##       &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;
## 1     0.720  12.0 1.39e-46</code></pre>
<p>We see the model strongly predicts yield, but not as strongly as <span class="math inline">\(k\)</span>-Nearest Neighbors.</p>
</div>
<div id="random-forest" class="section level3 hasAnchor" number="13.4.6">
<h3><span class="header-section-number">13.4.6</span> Random Forest<a href="machine-learning.html#random-forest" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>There is one last problem with our classification tree. What if the feature selected as the root of our model is the best for the dataset to which we will apply the trained model? In that case, our model may not be accurate. In addition, once a feature is chosen as the root of a classification model, the features and splits in the remainder of the model also become constrained.</p>
<p>This not only limits model performance, but also gives us an incomplete sense of how each feature truly effected the response variable. What if we started with a different root feature? What kind of model would we get?</p>
<p>This question can be answered with an <em>ensemble</em> of classification trees. This ensemble is known as a <em>random forest</em> (i.e. a collection of many, many classification trees). The random forest generates hundreds of models, each starting with a randomly selected feature as the root. These models are then combined as an ensemble to make predictions for the new population.</p>
<div class="sourceCode" id="cb1018"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1018-1"><a href="machine-learning.html#cb1018-1" aria-hidden="true" tabindex="-1"></a>rfFit <span class="ot">=</span> <span class="fu">train</span>(corn <span class="sc">~</span> .,</span>
<span id="cb1018-2"><a href="machine-learning.html#cb1018-2" aria-hidden="true" tabindex="-1"></a>                <span class="at">data =</span> trainData,</span>
<span id="cb1018-3"><a href="machine-learning.html#cb1018-3" aria-hidden="true" tabindex="-1"></a>                <span class="at">method =</span> <span class="st">&quot;rf&quot;</span>,</span>
<span id="cb1018-4"><a href="machine-learning.html#cb1018-4" aria-hidden="true" tabindex="-1"></a>                <span class="at">trControl =</span> ctrl)</span>
<span id="cb1018-5"><a href="machine-learning.html#cb1018-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1018-6"><a href="machine-learning.html#cb1018-6" aria-hidden="true" tabindex="-1"></a>rfFit</span></code></pre></div>
<pre><code>## Random Forest 
## 
## 665 samples
##   7 predictor
## 
## No pre-processing
## Resampling: Cross-Validated (10 fold, repeated 1 times) 
## Summary of sample sizes: 598, 600, 600, 601, 599, 597, ... 
## Resampling results across tuning parameters:
## 
##   mtry  RMSE       Rsquared   MAE     
##   2     10.559385  0.8360096  7.868132
##   4      9.983132  0.8482014  7.327396
##   7      9.788704  0.8519592  7.159581
## 
## RMSE was used to select the optimal model using the smallest value.
## The final value used for the model was mtry = 7.</code></pre>
<p>If we compare our model results to the individual classification tree above, and to the performance of our nearest-neighbor model, we see the random forest provides a better fit of the data.</p>
<div class="sourceCode" id="cb1020"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1020-1"><a href="machine-learning.html#cb1020-1" aria-hidden="true" tabindex="-1"></a>testData<span class="sc">$</span>corn_pred <span class="ot">=</span> <span class="fu">predict</span>(rfFit, testData)</span>
<span id="cb1020-2"><a href="machine-learning.html#cb1020-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1020-3"><a href="machine-learning.html#cb1020-3" aria-hidden="true" tabindex="-1"></a>testData <span class="sc">%&gt;%</span></span>
<span id="cb1020-4"><a href="machine-learning.html#cb1020-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x=</span>corn, <span class="at">y=</span>corn_pred)) <span class="sc">+</span></span>
<span id="cb1020-5"><a href="machine-learning.html#cb1020-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>() <span class="sc">+</span></span>
<span id="cb1020-6"><a href="machine-learning.html#cb1020-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_smooth</span>(<span class="at">method =</span> <span class="st">&quot;lm&quot;</span>) <span class="sc">+</span></span>
<span id="cb1020-7"><a href="machine-learning.html#cb1020-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">x=</span><span class="st">&quot;Actual Yield (bu)&quot;</span>, <span class="at">y=</span><span class="st">&quot;Predicted Yield (bu)&quot;</span>)</span></code></pre></div>
<pre><code>## `geom_smooth()` using formula &#39;y ~ x&#39;</code></pre>
<p><img src="13-Machine-Learning_files/figure-html/unnamed-chunk-30-1.png" width="672" /></p>
<div class="sourceCode" id="cb1022"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1022-1"><a href="machine-learning.html#cb1022-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(broom)</span>
<span id="cb1022-2"><a href="machine-learning.html#cb1022-2" aria-hidden="true" tabindex="-1"></a>model <span class="ot">=</span> <span class="fu">lm</span>(corn_pred<span class="sc">~</span>corn, <span class="at">data=</span>testData)</span>
<span id="cb1022-3"><a href="machine-learning.html#cb1022-3" aria-hidden="true" tabindex="-1"></a><span class="fu">glance</span>(model) <span class="sc">%&gt;%</span></span>
<span id="cb1022-4"><a href="machine-learning.html#cb1022-4" aria-hidden="true" tabindex="-1"></a>  dplyr<span class="sc">::</span><span class="fu">select</span>(r.squared, sigma, p.value) </span></code></pre></div>
<pre><code>## # A tibble: 1 x 3
##   r.squared sigma  p.value
##       &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;
## 1     0.810  9.51 3.09e-60</code></pre>
</div>
<div id="feature-importance" class="section level3 hasAnchor" number="13.4.7">
<h3><span class="header-section-number">13.4.7</span> Feature Importance<a href="machine-learning.html#feature-importance" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>When we use random forest analysis, there is no one tree to review – the forest, after all, is a collection of many hundreds or thousands of trees. There is also no longer one set of rules to explain the predicted values, since all trees were used to predict corn yield.</p>
<p>Random forest analysis, however, allows us to rank the features by importance. Feature importance is calculated from the number of times a feature occurs in the top branches of tree and the most frequent level at which a feature first occurs in the model.</p>
<p>We see in the barplot below the feature importance in our random forest model. Precipitaion was by far the single most important feature in predicting yield. The next most important feature, as we might expect, was cumulative growing degree days.</p>
<div class="sourceCode" id="cb1024"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1024-1"><a href="machine-learning.html#cb1024-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(vip)</span></code></pre></div>
<pre><code>## 
## Attaching package: &#39;vip&#39;</code></pre>
<pre><code>## The following object is masked from &#39;package:utils&#39;:
## 
##     vi</code></pre>
<div class="sourceCode" id="cb1027"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1027-1"><a href="machine-learning.html#cb1027-1" aria-hidden="true" tabindex="-1"></a><span class="fu">vip</span>(rfFit)</span></code></pre></div>
<p><img src="13-Machine-Learning_files/figure-html/unnamed-chunk-32-1.png" width="672" /></p>
<p>After that, we might wonder whether soil biology or texture might be the next most important feature. Our feature importance ranking suggests that organic matter is slightly more important than clay, followed by water holding capacity. We might infer that soil fertility which increases with organic matter and clay, was more important than drainage, which would be predicted by water holding capacity and our last variable, sand.</p>
</div>
</div>
<div id="summary-2" class="section level2 hasAnchor" number="13.5">
<h2><span class="header-section-number">13.5</span> Summary<a href="machine-learning.html#summary-2" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>With that, you have been introduced to three (four, if we include random forest) powerful machine learning tools. I don’t expect that you will personally use these tools on a regular basis; thus, this lesson is not hands-on like other lessons. It is most important that you are aware there is a whole world of statistics beyond what we have learned this semester, beyond designed experiments and the testing of treatments one-plot-at-a-time.</p>
<p>This is not to disparage or minimize the statistics we learned earlier in this course. Controlled, designed-experiments will always be key in testing new practices or products. They will be how products are screened in the greenhouse, in the test plot, in the side-by-side trial on-farm. They are critical.</p>
<p>But for big, messy problems, like predicting yield (or product performance) county-by-county, machine learning tools are very cool. They are behind the hybrid recommended for your farm, the nitrogen rate recommended for your sidedress. They are way cool and offer wicked insights and I believe it is worth your while – as agriculutral scientists – to be aware of them.</p>

<div class="sourceCode" id="cb1028"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1028-1"><a href="machine-learning.html#cb1028-1" aria-hidden="true" tabindex="-1"></a><span class="co"># knitr::opts_chunk$set(cache = F)</span></span>
<span id="cb1028-2"><a href="machine-learning.html#cb1028-2" aria-hidden="true" tabindex="-1"></a><span class="co"># write(&quot;TMPDIR = &#39;./temp&#39;&quot;, file=file.path(Sys.getenv(&#39;R_USER&#39;),</span></span>
<span id="cb1028-3"><a href="machine-learning.html#cb1028-3" aria-hidden="true" tabindex="-1"></a><span class="co"># &#39;.Renviron&#39;))</span></span></code></pre></div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="spatial-statistics.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="putting-it-all-together.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/USERNAME/REPO/edit/BRANCH/13-Machine-Learning.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["_main.pdf", "_main.epub"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
